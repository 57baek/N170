{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.10.15\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and Keras 2.15.0\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Flatten, Dense)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Suppress TensorFlow warnings (optional)\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-021/Epochs_sub-021.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-026/Epochs_sub-026.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-026\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-019\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-019/Epochs_sub-019.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-010/Epochs_sub-010.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-010\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-017\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-028\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-017/Epochs_sub-017.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-028/Epochs_sub-028.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-016/Epochs_sub-016.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-016\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-029\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-029/Epochs_sub-029.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-011/Epochs_sub-011.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-027/Epochs_sub-027.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-027\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-018/Epochs_sub-018.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-020/Epochs_sub-020.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-020\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-002/Epochs_sub-002.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-005/Epochs_sub-005.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-033/Epochs_sub-033.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-005\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-033\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-034\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-035\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-034/Epochs_sub-034.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-035/Epochs_sub-035.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-032\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-004\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-032/Epochs_sub-032.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-004/Epochs_sub-004.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-003\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-040\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-003/Epochs_sub-003.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-040/Epochs_sub-040.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-025\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-025/Epochs_sub-025.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-022/Epochs_sub-022.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-014\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-014/Epochs_sub-014.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-013/Epochs_sub-013.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-012\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-012/Epochs_sub-012.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-015/Epochs_sub-015.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-023\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-023/Epochs_sub-023.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-024/Epochs_sub-024.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-039\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-039/Epochs_sub-039.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-006/Epochs_sub-006.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-001\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-008\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-037\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-001/Epochs_sub-001.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-008/Epochs_sub-008.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-037/Epochs_sub-037.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-030\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-031\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-030/Epochs_sub-030.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-031/Epochs_sub-031.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-009\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-009/Epochs_sub-009.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-036/Epochs_sub-036.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Loading the processed data: sub-038\n",
      "\n",
      "\n",
      "***** Loading the processed data: sub-007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-038/Epochs_sub-038.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
      "/var/folders/dh/551p960n6mv9t9byzj8pp0640000gp/T/ipykernel_16150/3738345600.py:34: RuntimeWarning: This filename (/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed/sub-007/Epochs_sub-007.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# Define the data directory where subject folders are located\n",
    "data_dir = '/Users/BAEK/Code/neurEx/data/N170/Data_Preprocessed'\n",
    "\n",
    "# List all subject folders\n",
    "subject_folders = [sub for sub in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, sub))]\n",
    "'''\n",
    "subjects = []\n",
    "for sub in os.listdir(data_dir):\n",
    "    if os.path.isdir(os.path.join(data_dir, sub)):\n",
    "        subjects.append(sub)\n",
    "'''\n",
    "\n",
    "# Initialize lists to hold data and labels from all subjects\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# Loop over each subject folder\n",
    "for subject in subject_folders:\n",
    "    \n",
    "    subject_data_dir = os.path.join(data_dir, subject)\n",
    "    \n",
    "    # Construct file paths for the subject's data\n",
    "    X_file = os.path.join(subject_data_dir, f'Epochs_{subject}.fif')\n",
    "    \n",
    "    # Check if data files exist\n",
    "    if os.path.exists(X_file):\n",
    "        \n",
    "        print()\n",
    "        print(f'***** Loading the processed data: {subject}')\n",
    "        print()\n",
    "        \n",
    "        # Load the data\n",
    "        X_subject = mne.read_epochs(X_file, preload=True, verbose=False)\n",
    "        y_subject = X_subject.events[:, 2]\n",
    "        \n",
    "        # Append to the list\n",
    "        X_sub_data = X_subject.get_data()\n",
    "        X_list.append(X_sub_data)\n",
    "        y_list.append(y_subject)\n",
    "        \n",
    "    else:\n",
    "        print()\n",
    "        print(f'***** Data file Does Not Exist: {subject}')\n",
    "        print()\n",
    "\n",
    "# Ensure that at least one subject has been loaded\n",
    "if len(X_list) == 0:\n",
    "    print()\n",
    "    raise ValueError(\"No data was loaded. Please check your data directory and files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P8 (Right Parietal): \n",
    "\n",
    "The parietal lobe is involved in sensory processing and the integration of sensory information. Specifically, the right parietal region is important for spatial awareness, visual processing, and attention. It is often engaged in tasks related to visual perception and sensory-motor coordination.\n",
    "\n",
    "### PO8 (Right Parieto-Occipital): \n",
    "\n",
    "The occipital lobe is the primary visual processing center in the brain, and the parietal lobe integrates sensory information. The PO8 region is crucial for processing visual information from both the environment and sensory input. It is especially important in the recognition of objects, including faces, and is involved in spatial processing and attention to visual stimuli.\n",
    "\n",
    "### O2 (Right Occipital): \n",
    "\n",
    "The occipital lobe is the brain’s main area for visual processing, including perception of visual stimuli such as shapes, colors, and faces. The right occipital lobe is particularly active during tasks related to visual processing and recognition of visual patterns.\n",
    "\n",
    "### P10 (Right Parietal-Temporal): \n",
    "\n",
    "The parietal lobe is involved in sensory integration, spatial awareness, and attention, while the temporal lobe is important for processing sensory input, especially auditory and visual information. The temporal lobe is heavily involved in memory, recognition, and face processing.\n",
    "\n",
    "\n",
    "        a = np.array([[1, 2], \n",
    "                    [3, 4]])\n",
    "        b = np.array([[5, 6], \n",
    "                    [7, 8]])\n",
    "\n",
    "        np.concatenate((a, b), axis=0)\n",
    "        # [[1, 2],\n",
    "        #  [3, 4],\n",
    "        #  [5, 6],\n",
    "        #  [7, 8]]\n",
    "\n",
    "        np.vstack((a,b))\n",
    "        # [[1, 2],\n",
    "        #  [3, 4],\n",
    "        #  [5, 6],\n",
    "        #  [7, 8]]\n",
    "\n",
    "        np.concatenate((a, b), axis=1)\n",
    "        # [[1 2 5 6]\n",
    "        #  [3 4 7 8]]\n",
    "\n",
    "        a = np.array([[[1, 2], \n",
    "                    [3, 4]]])  # Shape: (1, 2, 2)\n",
    "\n",
    "        b = np.array([[[5, 6], \n",
    "                    [7, 8]]])  # Shape: (1, 2, 2)\n",
    "\n",
    "        np.concatenate((a, b), axis=2)\n",
    "        # [[[1, 2, 5, 6],\n",
    "        #   [3, 4, 7, 8]]]\n",
    "\n",
    "        np.vstack((a,b))\n",
    "        # [[[1, 2],\n",
    "        #   [3, 4]],\n",
    "        #\n",
    "        #   [[5, 6],\n",
    "        #    [7, 8]]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Mock EEG data with shape (3 samples, 4 channels, 5 timepoints)\n",
    "        X = np.array([\n",
    "            [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20]],\n",
    "            [[21, 22, 23, 24, 25], [26, 27, 28, 29, 30], [31, 32, 33, 34, 35], [36, 37, 38, 39, 40]],\n",
    "            [[41, 42, 43, 44, 45], [46, 47, 48, 49, 50], [51, 52, 53, 54, 55], [56, 57, 58, 59, 60]]\n",
    "        ])\n",
    "        # Shape: (3 samples, 4 channels, 5 timepoints)\n",
    "\n",
    "        chan_idx = [1, 3]  # Select only channels 1 and 3\n",
    "\n",
    "        X = X[:, chan_idx, :]\n",
    "\n",
    "        # Extracts only the specified channels\n",
    "        X = np.array([\n",
    "            [[6, 7, 8, 9, 10], [16, 17, 18, 19, 20]],\n",
    "            [[26, 27, 28, 29, 30], [36, 37, 38, 39, 40]],\n",
    "            [[46, 47, 48, 49, 50], [56, 57, 58, 59, 60]]\n",
    "        ])\n",
    "        # Shape: (3 samples, 2 channels, 5 timepoints)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Labels for three subjects\n",
    "        y_list = [\n",
    "            np.array([0, 1, 0, 1]),  # Subject 1\n",
    "            np.array([1, 1, 0, 0]),  # Subject 2\n",
    "            np.array([0, 0, 1, 1])   # Subject 3\n",
    "        ]\n",
    "\n",
    "        np.concatenate(y_list, axis=0)\n",
    "        #[0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1])  # Shape: (12,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape before filtering: X=(23602, 4, 820), y=(23602,)\n",
      "Combined data shape after filtering: X=(6104, 4, 102), y=(6104,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.65535473e-07, -2.33084976e-07, -2.56221681e-07, -3.35722922e-07,\n",
       "       -4.71088408e-07, -6.60497783e-07, -9.00880692e-07, -1.18786204e-06,\n",
       "       -1.51586949e-06, -1.87812459e-06, -2.26676702e-06, -2.67311763e-06,\n",
       "       -3.08767533e-06, -3.50033306e-06, -3.90088247e-06, -4.27891778e-06,\n",
       "       -4.62421035e-06, -4.92699383e-06, -5.17809914e-06, -5.36951111e-06,\n",
       "       -5.49421166e-06, -5.54619740e-06, -5.52102088e-06, -5.41581248e-06,\n",
       "       -5.22933673e-06, -4.96202801e-06, -4.61585473e-06, -4.19435119e-06,\n",
       "       -3.70253657e-06, -3.14671301e-06, -2.53441977e-06, -1.87421798e-06,\n",
       "       -1.17549646e-06, -4.48325484e-07,  2.96874105e-07,  1.04961514e-06,\n",
       "        1.79936683e-06,  2.53584337e-06,  3.24924111e-06,  3.93041848e-06,\n",
       "        4.57091855e-06,  5.16319560e-06,  5.70080136e-06,  6.17847393e-06,\n",
       "        6.59201477e-06,  6.93824957e-06,  7.21520756e-06,  7.42191790e-06,\n",
       "        7.55842064e-06,  7.62581203e-06,  7.62584541e-06,  7.56124113e-06,\n",
       "        7.43529985e-06,  7.25198172e-06,  7.01556442e-06,  6.73096894e-06,\n",
       "        6.40343235e-06,  6.03826998e-06,  5.64088057e-06,  5.21691702e-06,\n",
       "        4.77197837e-06,  4.31159448e-06,  3.84124278e-06,  3.36638641e-06,\n",
       "        2.89221906e-06,  2.42390560e-06,  1.96652901e-06,  1.52481222e-06,\n",
       "        1.10321057e-06,  7.05951629e-07,  3.36879789e-07, -5.09837175e-10,\n",
       "       -3.03096621e-07, -5.68101464e-07, -7.93489514e-07, -9.77722762e-07,\n",
       "       -1.11973417e-06, -1.21901905e-06, -1.27566862e-06, -1.29047739e-06,\n",
       "       -1.26481735e-06, -1.20055294e-06, -1.10021996e-06, -9.66808793e-07,\n",
       "       -8.03887902e-07, -6.15385769e-07, -4.05521183e-07, -1.79039269e-07,\n",
       "        5.93880190e-08,  3.05053800e-07,  5.53013145e-07,  7.98585830e-07,\n",
       "        1.03721082e-06,  1.26473081e-06,  1.47720015e-06,  1.67114651e-06,\n",
       "        1.84364616e-06,  1.99222230e-06,  2.11499428e-06,  2.21065354e-06,\n",
       "        2.27832317e-06,  2.31764697e-06])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get channel names from the last loaded subject\n",
    "# This retrieves the names of the channels from the EEG data. These names represent the locations or positions of the EEG electrodes on the scalp, like ‘FP1’, ‘F3’, ‘P3’, etc.\n",
    "channels = X_subject.ch_names\n",
    "\n",
    "# Channels such as P8, PO8, O2, and P10 are situated over or near these regions in the parietal and occipital lobes, which are strongly involved in facial processing and visual stimuli processing.\n",
    "# Studies on the N170 often report that the most reliable and strongest N170 signals are found in right-lateralized occipital-temporal regions. \n",
    "coi = ['P8', 'PO8', 'O2', 'P10']\n",
    "\n",
    "# Creates a list of indices (chan_idx) for the channels of interest. \n",
    "# It finds the index (position) of each channel in the coi list from the full channels list.\n",
    "chan_idx = [channels.index(chan) for chan in coi] # [25, 27, 29, 26]\n",
    "'''\n",
    "chan_idx = []\n",
    "for chan in coi:\n",
    "    index = channels.index(chan)    # Get the index of the channel in the full list\n",
    "    chan_idx.append(index)          # Append the index to the list\n",
    "'''\n",
    "\n",
    "### Concatenate data from all subjects\n",
    "# X_list: This is a list containing the EEG data arrays for multiple subjects. Each array in X_list typically has a shape like (trials, channels, time_points).\n",
    "\n",
    "# NumPy arrays are multi-dimensional, and each dimension is associated with an axis:\n",
    "# 0 represents the rows (vertical direction).\n",
    "# 1 represents the columns (horizontal direction).\n",
    "# 2, 3, and beyond represent additional dimensions (for higher-dimensional arrays).\n",
    "\n",
    "# Combines the EEG data for all subjects along the trials dimension. Same as X = np.vstack(X_list) when concatenate(X_list, axis = 0)\n",
    "# After concatenation, X will contain all trials from all subjects in a single array.\n",
    "# If, for example, each subject’s data has 100 trials, and there are 10 subjects, then X will have a shape (1000, channels, time_points).\n",
    "X = np.concatenate(X_list, axis = 0) # will combine all x of X(x,y,z)\n",
    "\n",
    "# A slicing operation on the X array, and it extracts specific subsets of the data along its second axis (dimensions).\n",
    "X = X[:, chan_idx, :]  # will change the y of X(x,y,z)\n",
    "\n",
    "# Combines all the individual y arrays in y_list into a single, larger 1D or 2D array along the first axis (axis=0)\n",
    "y = np.concatenate(y_list, axis = 0)\n",
    "\n",
    "print(f'Combined data shape before filtering: X={X.shape}, y={y.shape}')\n",
    "\n",
    "### Filter stimulus events\n",
    "stimulus_labels = [1, 2]  # 1: Face, 2: Car\n",
    "# Checks each element in y to see if it is in the stimulus_labels list.\n",
    "# Returns a boolean array (stimulus_mask) of the same length as y.\n",
    "stimulus_mask = np.isin(y, stimulus_labels)\n",
    "X = X[stimulus_mask] # will change the x of X(x,y,z)\n",
    "y = y[stimulus_mask]\n",
    "\n",
    "# Adjust labels to start from 0\n",
    "y = y - 1\n",
    "\n",
    "# Focus on the N170 response by selecting data around 170 ms \n",
    "# Selecting Data Around the N170 Response (time window selection)\n",
    "# This defines the time window you’re interested in, i.e., 100ms to 200ms after the stimulus.\n",
    "# This means you are interested in data collected from 0.1 seconds to 0.2 seconds after the stimulus\n",
    "tmin, tmax = 0.10, 0.2   # 100 ms to 200 ms\n",
    "times = X_subject.times  # Time vector from the epochs\n",
    "# Filter the times so that only the times that lies between tmin and tmax stay\n",
    "time_mask = (times >= tmin) & (times <= tmax)\n",
    "X_focused = X[:, :, time_mask]\n",
    "\n",
    "print(f'Combined data shape after filtering: X={X_focused.shape}, y={y.shape}')\n",
    "\n",
    "X_focused[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6104, 4, 102)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-2.65535473e-07, -2.33084976e-07, -2.56221681e-07, ...,\n",
       "          2.21065354e-06,  2.27832317e-06,  2.31764697e-06],\n",
       "        [ 1.52646923e-06,  1.55423677e-06,  1.55282390e-06, ...,\n",
       "          7.41099594e-07,  9.07904801e-07,  1.05732774e-06],\n",
       "        [-7.22147654e-06, -7.38670919e-06, -7.53458689e-06, ...,\n",
       "         -4.07241284e-07, -2.46243744e-07, -8.30428524e-08],\n",
       "        [ 5.03884314e-06,  5.25305508e-06,  5.36733101e-06, ...,\n",
       "          8.15229414e-06,  8.16074655e-06,  8.08678148e-06]],\n",
       "\n",
       "       [[-1.47737646e-06, -1.67229044e-06, -1.82316827e-06, ...,\n",
       "          3.79423474e-06,  3.87306189e-06,  3.89641451e-06],\n",
       "        [-6.68424724e-07, -6.97640298e-07, -7.08379923e-07, ...,\n",
       "         -1.09889269e-06, -8.65716336e-07, -6.51313899e-07],\n",
       "        [-7.48422084e-07, -9.07281396e-07, -1.03889632e-06, ...,\n",
       "         -8.53871045e-07, -6.54174386e-07, -4.62437718e-07],\n",
       "        [-4.21238231e-06, -4.83159445e-06, -5.40723513e-06, ...,\n",
       "         -1.00315418e-08,  2.54398673e-07,  5.16855537e-07]],\n",
       "\n",
       "       [[-1.58109676e-06, -1.59698379e-06, -1.62114536e-06, ...,\n",
       "          9.26621018e-07,  8.29454003e-07,  7.13630019e-07],\n",
       "        [-4.45750903e-06, -4.44266032e-06, -4.42975663e-06, ...,\n",
       "          2.08250570e-06,  2.06292653e-06,  2.02929306e-06],\n",
       "        [-7.26043127e-06, -7.20383261e-06, -7.18244503e-06, ...,\n",
       "         -4.33158683e-06, -4.11023377e-06, -3.90746092e-06],\n",
       "        [ 2.11854290e-06,  1.82837104e-06,  1.53560948e-06, ...,\n",
       "         -2.51281231e-07, -6.76515637e-07, -1.08717251e-06]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.41856908e-06, -2.79457211e-06, -3.19109940e-06, ...,\n",
       "         -2.42563176e-06, -2.00228238e-06, -1.71271193e-06],\n",
       "        [-6.96223853e-07, -6.26056312e-07, -4.53751146e-07, ...,\n",
       "          5.96395443e-06,  6.74311493e-06,  7.50302313e-06],\n",
       "        [ 3.95886778e-07,  4.29583042e-07,  5.65602897e-07, ...,\n",
       "          5.21327113e-06,  5.82447718e-06,  6.48118637e-06],\n",
       "        [ 1.04313421e-06,  8.32708297e-07,  5.80027221e-07, ...,\n",
       "          8.27353857e-06,  8.85092542e-06,  9.35755155e-06]],\n",
       "\n",
       "       [[ 2.70915841e-06,  3.19704007e-06,  3.67138456e-06, ...,\n",
       "          2.98408722e-06,  2.79047846e-06,  2.55993509e-06],\n",
       "        [ 7.13312815e-06,  7.51920269e-06,  7.85860489e-06, ...,\n",
       "          3.97416948e-06,  4.26586150e-06,  4.53748416e-06],\n",
       "        [ 1.17926740e-05,  1.24428749e-05,  1.30259113e-05, ...,\n",
       "          1.09956960e-05,  1.13510370e-05,  1.16468868e-05],\n",
       "        [ 1.89418709e-06,  2.10378980e-06,  2.27546334e-06, ...,\n",
       "          7.46997593e-07,  8.92711339e-07,  1.05645668e-06]],\n",
       "\n",
       "       [[ 3.04348611e-06,  2.54423522e-06,  1.99804985e-06, ...,\n",
       "         -7.77291475e-07, -1.00802660e-06, -1.27954244e-06],\n",
       "        [ 7.12797640e-06,  7.01452778e-06,  6.88476227e-06, ...,\n",
       "          1.06899452e-05,  1.06339998e-05,  1.05105028e-05],\n",
       "        [ 8.32707117e-06,  8.15730665e-06,  7.90702293e-06, ...,\n",
       "          9.06495855e-06,  8.92041776e-06,  8.70979688e-06],\n",
       "        [ 5.80448149e-06,  5.73677491e-06,  5.66561602e-06, ...,\n",
       "          9.76266858e-06,  9.35750959e-06,  8.83536909e-06]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_focused.shape)\n",
    "\n",
    "X_focused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. X\n",
    "\n",
    "NumPy arrays are multidimensional data structures. Each dimension corresponds to a different aspect of the data.\n",
    "\n",
    "First Dimension (595):\n",
    "•\tRepresents 595 epochs (or trials) in your experiment. Each epoch corresponds to a single trial where the EEG data is collected during a stimulus presentation or event.\n",
    "\t•\tThis dimension will vary as you add more subjects or trials, but each epoch will contain data in the format specified by the next two dimensions.\n",
    "\n",
    "Second Dimension (33):\n",
    "•\tRepresents 33 EEG channels (electrodes) capturing data from various scalp locations. Each channel corresponds to a different electrode that records electrical activity from the brain.\n",
    "•\tThese 33 channels provide spatial information about where the brain activity is occurring on the scalp.\n",
    "•\tThis dimension will remain the same across all subjects, as you are using the same number of electrodes to capture the data.\n",
    "\n",
    "Third Dimension (820):\n",
    "•\tRepresents 820 time points for each epoch. These are the EEG values recorded over time during each trial. The EEG signal is continuous, and the time points correspond to the sampling rate and the duration of the recording.\n",
    "•\tThis dimension also remains the same across all subjects, as the data is sampled at the same rate and duration for each trial.\n",
    "\n",
    "2. For example \n",
    "\n",
    "import numpy as np\n",
    "box = np.random.rand(10, 20, 5)  # Generate random values in a (10, 20, 5) shape\n",
    "print(box)\n",
    "\n",
    "  array([[[0.72, 0.43, 0.99, 0.58, 0.16],  # Epoch 1, Channel 1\n",
    "          [0.63, 0.34, 0.88, 0.44, 0.01],  # Epoch 1, Channel 2\n",
    "          ...\n",
    "          [0.56, 0.11, 0.98, 0.75, 0.22]], # Epoch 1, Channel 20\n",
    "         [[0.89, 0.22, 0.78, 0.33, 0.55],  # Epoch 2, Channel 1\n",
    "          [0.71, 0.48, 0.64, 0.88, 0.19],\n",
    "          ...\n",
    "          [0.12, 0.95, 0.72, 0.66, 0.23]], # Epoch 2, Channel 20\n",
    "          ...\n",
    "         [[0.82, 0.73, 0.61, 0.88, 0.34],  # Epoch 10, Channel 1\n",
    "          [0.44, 0.56, 0.77, 0.93, 0.18],\n",
    "          ...\n",
    "          [0.62, 0.29, 0.38, 0.99, 0.41]]  # Epoch 10, Channel 20\n",
    "        ])\n",
    "\n",
    "Step 1: Mapping xx to Excel Files\n",
    "\n",
    "The structure of your EEG data (xx) can be thought of as:\n",
    "\t•\tRows in the Excel sheet (y-axis): These correspond to the 33 EEG channels.\n",
    "\t•\tColumns in the Excel sheet (x-axis): These correspond to the 820 time points.\n",
    "\t•\tMultiple Excel sheets (z-axis): Each sheet corresponds to one epoch (595 epochs total).\n",
    "\n",
    "Imagine one epoch of data as a single Excel file. For Epoch 1, you have a 33×820 table:\n",
    "\n",
    "Channel/Time\t t1\t    t2\t t3\t  …\tt820\n",
    "         Ch1\t0.12\t-0.34\t0.56\t…\t0.44\n",
    "         Ch2\t0.15\t-0.23\t0.54\t…\t0.39\n",
    "         Ch3\t0.10\t-0.18\t0.45\t…\t0.35\n",
    "          …\t   …\t    …\t   …\t  …\t …\n",
    "         Ch33\t0.22\t-0.28\t0.67\t…\t0.50\n",
    "\n",
    "Step 3: Visualizing Across Epochs\n",
    "\n",
    "Now, you stack 595 Excel files one on top of the other. Each file has the same dimensions (33×820), but the data values inside them differ because each epoch captures a slightly different response (depending on the stimulus, noise, or other factors).\n",
    "\n",
    "Step 4: Linking to the Labels (yy)\n",
    "\n",
    "Each Excel file (epoch) corresponds to a single value in yy, which indicates the event type or condition. For example:\n",
    "\n",
    "Epoch\tLabel (Event Type)\n",
    "  1\t    1   (Face stimulus)\n",
    "  2\t    2   (Car stimulus)\n",
    "  3    \t1   (Face stimulus)\n",
    "  …\t    …       …\n",
    " 595   \t4   (Scrambled car)\n",
    "\n",
    "Step 5: Overall Data Structure\n",
    "\n",
    "Think of your data as 595 Excel files:\n",
    "•\tEach file has 33 rows (EEG channels) and 820 columns (time points).\n",
    "•\tYou also have a separate label file (yy) that tells you what condition or event type corresponds to each Excel file.\n",
    "•\tFor the table we created for epoch 1 with a 33×820 table, it's name is 1 (Face stimulus) according to yy.\n",
    "•\tx (first dimension): Epochs (each epoch is like a “table”).\n",
    "•\ty (second dimension): Channels (columns in the table).\n",
    "•\tz (third dimension): Time points (rows in the table).\n",
    "\n",
    "3. Normalization (Z-score normalization)\n",
    "\n",
    "Normalization is a standard pre-processing step in machine learning and signal processing to ensure that all features (in this case, the EEG signal values across all epochs) are on the same scale. Without normalization, features with larger scales (like EEG channels with higher amplitude signals) might dominate over features with smaller scales (like signals with less variance). This can make the training of machine learning models less effective.\n",
    "\n",
    "Z-score Normalization is a type of standardization, where we scale the data such that it has a mean of 0 and a standard deviation of 1. \n",
    "\n",
    "Z = \\frac{X - \\mu}{\\sigma}\n",
    "\n",
    "Where:\n",
    "•\tX is the original data point (EEG signal value at a specific time and channel),\n",
    "•\t\\mu is the mean of the data (average signal value),\n",
    "•\t\\sigma is the standard deviation (how much the data varies from the mean).\n",
    "\n",
    "This normalization ensures that the data has a consistent scale, which helps improve the performance of machine learning algorithms, especially deep learning models.\n",
    "\n",
    "4. For example \n",
    "\n",
    "x = [\n",
    "    [1, 2, 3, 4],  # Epoch 1\n",
    "    [5, 6, 7, 8],  # Epoch 2\n",
    "    [9, 10, 11, 12] # Epoch 3\n",
    "]\n",
    "\n",
    "Here, each row corresponds to an epoch (a trial), and each column corresponds to a feature (in this case, 4 features).\n",
    "\t•\tEpoch 1: [1, 2, 3, 4]\n",
    "\t•\tEpoch 2: [5, 6, 7, 8]\n",
    "\t•\tEpoch 3: [9, 10, 11, 12]\n",
    "\n",
    "Step-by-Step Z-Score Normalization\n",
    "\n",
    "\n",
    "Mean and Standard Deviation Calculation:\n",
    "For each feature (column), we calculate the mean and standard deviation:\n",
    "\t\n",
    "  •\tMean of Feature 1:  \\frac{1 + 5 + 9}{3} = 5 \n",
    "\t•\tMean of Feature 2:  \\frac{2 + 6 + 10}{3} = 6 \n",
    "\t•\tMean of Feature 3:  \\frac{3 + 7 + 11}{3} = 7 \n",
    "\t•\tMean of Feature 4:  \\frac{4 + 8 + 12}{3} = 8 \n",
    "\t\n",
    "  •\tStandard Deviation of Feature 1:\n",
    " \\sqrt{\\frac{(1-5)^2 + (5-5)^2 + (9-5)^2}{3}} = 3.464 \n",
    "\t•\tStandard Deviation of Feature 2:\n",
    " \\sqrt{\\frac{(2-6)^2 + (6-6)^2 + (10-6)^2}{3}} = 3.464 \n",
    "\t•\tStandard Deviation of Feature 3:\n",
    " \\sqrt{\\frac{(3-7)^2 + (7-7)^2 + (11-7)^2}{3}} = 3.464 \n",
    "\t•\tStandard Deviation of Feature 4:\n",
    " \\sqrt{\\frac{(4-8)^2 + (8-8)^2 + (12-8)^2}{3}} = 3.464 \n",
    "\n",
    "Apply Z-Score Normalization:\n",
    "\n",
    "Z-score normalization is applied by the formula:\n",
    "\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "\n",
    "where:\n",
    "\t•\t x  is the original value,\n",
    "\t•\t \\mu  is the mean of the feature,\n",
    "\t•\t \\sigma  is the standard deviation of the feature.\n",
    "For Epoch 1, applying Z-score to each feature:\n",
    "\t•\tFeature 1:  \\frac{1 - 5}{3.464} = -1.155 \n",
    "\t•\tFeature 2:  \\frac{2 - 6}{3.464} = -1.155 \n",
    "\t•\tFeature 3:  \\frac{3 - 7}{3.464} = -1.155 \n",
    "\t•\tFeature 4:  \\frac{4 - 8}{3.464} = -1.155 \n",
    "Normalized Epoch 1: [-1.155, -1.155, -1.155, -1.155]\n",
    "\n",
    "Similarly, apply the Z-score normalization for Epoch 2 and Epoch 3:\n",
    "\n",
    "Normalized Epoch 2:\n",
    "\t•\tFeature 1:  \\frac{5 - 5}{3.464} = 0 \n",
    "\t•\tFeature 2:  \\frac{6 - 6}{3.464} = 0 \n",
    "\t•\tFeature 3:  \\frac{7 - 7}{3.464} = 0 \n",
    "\t•\tFeature 4:  \\frac{8 - 8}{3.464} = 0 \n",
    "Normalized Epoch 2: [0, 0, 0, 0]\n",
    "\n",
    "Normalized Epoch 3:\n",
    "\t•\tFeature 1:  \\frac{9 - 5}{3.464} = 1.155 \n",
    "\t•\tFeature 2:  \\frac{10 - 6}{3.464} = 1.155 \n",
    "\t•\tFeature 3:  \\frac{11 - 7}{3.464} = 1.155 \n",
    "\t•\tFeature 4:  \\frac{12 - 8}{3.464} = 1.155 \n",
    "Normalized Epoch 3: [1.155, 1.155, 1.155, 1.155]\n",
    "\n",
    "x = [\n",
    "    [-1.155, -1.155, -1.155, -1.155],  # Epoch 1\n",
    "    [0,     0,     0,     0    ],     # Epoch 2\n",
    "    [1.155, 1.155, 1.155, 1.155]      # Epoch 3\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBefore the np.newaxis operation:\\n•\\tThe shape of your array X_normalized is something like (samples, channels, time_points).\\n•\\tThe data is structured in a 3D array with samples as the first dimension, channels as the second dimension, and time points as the third dimension.\\n•\\tEach time point for every channel in every sample is a scalar value (real number), and the array looks like this:\\n array([[[ 3.24529244e-01,  4.11996053e-01,  4.91609174e-01, ...],\\n         [ 1.84049422e-01,  2.35332724e-01,  2.83798373e-01, ...],\\n         [-2.38773378e-01, -2.04526201e-01, -1.74333753e-01, ...],\\n         [ 4.25627014e-01,  4.73232880e-01,  5.16544162e-01, ...]],\\n\\n        [[-2.96749702e-01, -2.86137936e-01, -2.64685230e-01, ...],\\n         [-4.21221455e-01, -4.03617815e-01, -3.79205829e-01, ...],\\n         [ 2.98252681e-01,  2.99339691e-01,  3.04971293e-01, ...],\\n         [-2.24049276e-02, -2.27949784e-02, -2.29925722e-02, ...]],\\n\\n        [[-4.12923850e-01, -3.98015639e-01, -3.82177900e-01, ...], \\n        ...\\n\\nAfter the np.newaxis operation:\\n•\\tThe shape changes to (samples, channels, time_points, 1).\\n•\\tThis operation wraps each individual value (the scalar for each time point) into its own array (of size 1) and adds a new dimension.\\n•\\tNow, each time point for each channel/sample is a 1D array (each value in your array is now encapsulated as a single-element list or array).\\narray([[[[ 3.24529244e-01],\\n         [ 4.11996053e-01],\\n         [ 4.91609174e-01],\\n         ...],\\n \\n        [[ 1.84049422e-01],\\n         [ 2.35332724e-01],\\n         [ 2.83798373e-01],\\n         ...],\\n\\n        [[-2.38773378e-01],\\n         [-2.04526201e-01],\\n         [-1.74333753e-01],\\n         ...],\\n\\n        [[ 4.25627014e-01],\\n         [ 4.73232880e-01],\\n         [ 5.16544162e-01],\\n         ...]]], \\n       ...\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize data per channel (not global)\n",
    "def normalize_per_channel(X_data):\n",
    "    # Creates an array of the same shape as X_data, but with all values initialized to zero. \n",
    "    # The purpose of this array is to store the normalized data.\n",
    "    X_norm = np.zeros_like(X_data)\n",
    "    for i in range(X_data.shape[1]):  # Loop over the range channels: total 4 times from i=0 to i=3\n",
    "        scaler = StandardScaler()\n",
    "        # X_channel will have a shape of (6104, 61), representing the data for all trials across all time points for channel i\n",
    "        # X_data = [[[a1], [a2], [a3], [a4]], \n",
    "        #           [[b1], [b2], [b3], [b4]], \n",
    "        #           [[c1], [c2], [c3], [c4]]]\n",
    "        # X_channel = [[a1],[b1],[c1]]\n",
    "        X_channel = X_data[:, i, :]\n",
    "        # First, it computes the mean and standard deviation for each feature (each column) in X_channel. \n",
    "        # After calculating the mean and standard deviation, it transforms the data by subtracting the mean and dividing by the standard deviation\n",
    "        X_norm[:, i, :] = scaler.fit_transform(X_channel)\n",
    "    return X_norm\n",
    "\n",
    "X_normalized = normalize_per_channel(X_focused)\n",
    "\n",
    "# Reshape data for Conv2D (samples, channels, times, depth/feature)\n",
    "# A typical CNN expects the input to be in a 4D shape: (samples, height, width, channels) or (batch_size, rows, columns, channels). \n",
    "# This is because CNNs are designed to process multi-dimensional data (images or time series) in terms of both spatial and channel dimensions.\n",
    "# The additional dimension (1) indicates that each sample is being treated as having a single channel (similar to grayscale images). \n",
    "# In other words, you’re telling the model that each time sample has just one feature (or one channel)\n",
    "# By reshaping it to (1000, 64, 50, 1), you’re essentially telling the model that for each of the 1000 samples, you have 64 channels and 50 time samples, and you are treating each one as a single-channel input.\n",
    "# Convolutional layers typically expect a 4D input, where the last dimension (1 in this case) represents the number of channels. By reshaping the input, the model can apply convolutional operations over the time and channel dimensions correctly, learning temporal patterns from the data.\n",
    "X = X_normalized[..., np.newaxis]  # Shape: (samples, channels, times, 1)\n",
    "\n",
    "'''\n",
    "Before the np.newaxis operation:\n",
    "•\tThe shape of your array X_normalized is something like (samples, channels, time_points).\n",
    "•\tThe data is structured in a 3D array with samples as the first dimension, channels as the second dimension, and time points as the third dimension.\n",
    "•\tEach time point for every channel in every sample is a scalar value (real number), and the array looks like this:\n",
    " array([[[ 3.24529244e-01,  4.11996053e-01,  4.91609174e-01, ...],\n",
    "         [ 1.84049422e-01,  2.35332724e-01,  2.83798373e-01, ...],\n",
    "         [-2.38773378e-01, -2.04526201e-01, -1.74333753e-01, ...],\n",
    "         [ 4.25627014e-01,  4.73232880e-01,  5.16544162e-01, ...]],\n",
    "\n",
    "        [[-2.96749702e-01, -2.86137936e-01, -2.64685230e-01, ...],\n",
    "         [-4.21221455e-01, -4.03617815e-01, -3.79205829e-01, ...],\n",
    "         [ 2.98252681e-01,  2.99339691e-01,  3.04971293e-01, ...],\n",
    "         [-2.24049276e-02, -2.27949784e-02, -2.29925722e-02, ...]],\n",
    "\n",
    "        [[-4.12923850e-01, -3.98015639e-01, -3.82177900e-01, ...], \n",
    "        ...\n",
    "\n",
    "After the np.newaxis operation:\n",
    "•\tThe shape changes to (samples, channels, time_points, 1).\n",
    "•\tThis operation wraps each individual value (the scalar for each time point) into its own array (of size 1) and adds a new dimension.\n",
    "•\tNow, each time point for each channel/sample is a 1D array (each value in your array is now encapsulated as a single-element list or array).\n",
    "array([[[[ 3.24529244e-01],\n",
    "         [ 4.11996053e-01],\n",
    "         [ 4.91609174e-01],\n",
    "         ...],\n",
    " \n",
    "        [[ 1.84049422e-01],\n",
    "         [ 2.35332724e-01],\n",
    "         [ 2.83798373e-01],\n",
    "         ...],\n",
    "\n",
    "        [[-2.38773378e-01],\n",
    "         [-2.04526201e-01],\n",
    "         [-1.74333753e-01],\n",
    "         ...],\n",
    "\n",
    "        [[ 4.25627014e-01],\n",
    "         [ 4.73232880e-01],\n",
    "         [ 5.16544162e-01],\n",
    "         ...]]], \n",
    "       ...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data shape: X=(36624, 4, 102, 1), y=(36624,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[-0.31605341],\n",
       "         [-0.32619682],\n",
       "         [-0.34412525],\n",
       "         ...,\n",
       "         [ 0.32308469],\n",
       "         [ 0.31376044],\n",
       "         [ 0.30061452]],\n",
       "\n",
       "        [[-0.11632468],\n",
       "         [-0.12962838],\n",
       "         [-0.14648719],\n",
       "         ...,\n",
       "         [-0.11461243],\n",
       "         [-0.11596127],\n",
       "         [-0.11936317]],\n",
       "\n",
       "        [[-0.99076982],\n",
       "         [-1.01335294],\n",
       "         [-1.03497392],\n",
       "         ...,\n",
       "         [-0.41966924],\n",
       "         [-0.41317054],\n",
       "         [-0.40664085]],\n",
       "\n",
       "        [[ 0.28014587],\n",
       "         [ 0.29523614],\n",
       "         [ 0.29830576],\n",
       "         ...,\n",
       "         [ 0.88123175],\n",
       "         [ 0.86926358],\n",
       "         [ 0.84805751]]],\n",
       "\n",
       "\n",
       "       [[[-0.31605341],\n",
       "         [-0.32619682],\n",
       "         [-0.34412525],\n",
       "         ...,\n",
       "         [ 0.32308469],\n",
       "         [ 0.31376044],\n",
       "         [ 0.30061452]],\n",
       "\n",
       "        [[-0.11632468],\n",
       "         [-0.12962838],\n",
       "         [-0.14648719],\n",
       "         ...,\n",
       "         [-0.11461243],\n",
       "         [-0.11596127],\n",
       "         [-0.11936317]],\n",
       "\n",
       "        [[-0.99076982],\n",
       "         [-1.01335294],\n",
       "         [-1.03497392],\n",
       "         ...,\n",
       "         [-0.41966924],\n",
       "         [-0.41317054],\n",
       "         [-0.40664085]],\n",
       "\n",
       "        [[ 0.28014587],\n",
       "         [ 0.29523614],\n",
       "         [ 0.29830576],\n",
       "         ...,\n",
       "         [ 0.88123175],\n",
       "         [ 0.86926358],\n",
       "         [ 0.84805751]]],\n",
       "\n",
       "\n",
       "       [[[-0.31605341],\n",
       "         [-0.32619682],\n",
       "         [-0.34412525],\n",
       "         ...,\n",
       "         [ 0.32308469],\n",
       "         [ 0.31376044],\n",
       "         [ 0.30061452]],\n",
       "\n",
       "        [[-0.11632468],\n",
       "         [-0.12962838],\n",
       "         [-0.14648719],\n",
       "         ...,\n",
       "         [-0.11461243],\n",
       "         [-0.11596127],\n",
       "         [-0.11936317]],\n",
       "\n",
       "        [[-0.99076982],\n",
       "         [-1.01335294],\n",
       "         [-1.03497392],\n",
       "         ...,\n",
       "         [-0.41966924],\n",
       "         [-0.41317054],\n",
       "         [-0.40664085]],\n",
       "\n",
       "        [[ 0.28014587],\n",
       "         [ 0.29523614],\n",
       "         [ 0.29830576],\n",
       "         ...,\n",
       "         [ 0.88123175],\n",
       "         [ 0.86926358],\n",
       "         [ 0.84805751]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.14724637],\n",
       "         [ 0.06153537],\n",
       "         [-0.03029612],\n",
       "         ...,\n",
       "         [-0.08484899],\n",
       "         [-0.13388489],\n",
       "         [-0.18827787]],\n",
       "\n",
       "        [[ 0.53582672],\n",
       "         [ 0.50408512],\n",
       "         [ 0.47053158],\n",
       "         ...,\n",
       "         [ 0.99820162],\n",
       "         [ 0.96918532],\n",
       "         [ 0.93286022]],\n",
       "\n",
       "        [[ 0.9448504 ],\n",
       "         [ 0.91554101],\n",
       "         [ 0.87572254],\n",
       "         ...,\n",
       "         [ 0.79558872],\n",
       "         [ 0.76045166],\n",
       "         [ 0.71677905]],\n",
       "\n",
       "        [[ 0.36941967],\n",
       "         [ 0.35157934],\n",
       "         [ 0.33301539],\n",
       "         ...,\n",
       "         [ 1.06390236],\n",
       "         [ 1.00502702],\n",
       "         [ 0.93298101]]],\n",
       "\n",
       "\n",
       "       [[[ 0.14724637],\n",
       "         [ 0.06153537],\n",
       "         [-0.03029612],\n",
       "         ...,\n",
       "         [-0.08484899],\n",
       "         [-0.13388489],\n",
       "         [-0.18827787]],\n",
       "\n",
       "        [[ 0.53582672],\n",
       "         [ 0.50408512],\n",
       "         [ 0.47053158],\n",
       "         ...,\n",
       "         [ 0.99820162],\n",
       "         [ 0.96918532],\n",
       "         [ 0.93286022]],\n",
       "\n",
       "        [[ 0.9448504 ],\n",
       "         [ 0.91554101],\n",
       "         [ 0.87572254],\n",
       "         ...,\n",
       "         [ 0.79558872],\n",
       "         [ 0.76045166],\n",
       "         [ 0.71677905]],\n",
       "\n",
       "        [[ 0.36941967],\n",
       "         [ 0.35157934],\n",
       "         [ 0.33301539],\n",
       "         ...,\n",
       "         [ 1.06390236],\n",
       "         [ 1.00502702],\n",
       "         [ 0.93298101]]],\n",
       "\n",
       "\n",
       "       [[[ 0.15216996],\n",
       "         [ 0.0717804 ],\n",
       "         [-0.02049015],\n",
       "         ...,\n",
       "         [-0.07831029],\n",
       "         [-0.13914235],\n",
       "         [-0.18089616]],\n",
       "\n",
       "        [[ 0.54318211],\n",
       "         [ 0.48808046],\n",
       "         [ 0.45772378],\n",
       "         ...,\n",
       "         [ 0.98441888],\n",
       "         [ 0.96774153],\n",
       "         [ 0.94471681]],\n",
       "\n",
       "        [[ 0.95183587],\n",
       "         [ 0.90420543],\n",
       "         [ 0.88293889],\n",
       "         ...,\n",
       "         [ 0.80106924],\n",
       "         [ 0.74773913],\n",
       "         [ 0.72648837]],\n",
       "\n",
       "        [[ 0.37931299],\n",
       "         [ 0.35601833],\n",
       "         [ 0.33404865],\n",
       "         ...,\n",
       "         [ 1.0733584 ],\n",
       "         [ 1.00655105],\n",
       "         [ 0.95519562]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "def augment_data(X, y):\n",
    "    \n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "    \n",
    "    # Loop over each sample (trial or instance) in X\n",
    "    for i in range(X.shape[0]):\n",
    "        \n",
    "        # Original data\n",
    "        # For each sample, we first append the original data (X[i]) and the corresponding label (y[i]) to the X_augmented and y_augmented lists. \n",
    "        # This ensures that the original data is kept in the augmented dataset.\n",
    "        X_augmented.append(X[i])\n",
    "        y_augmented.append(y[i])\n",
    "\n",
    "        # Time-shifted data\n",
    "        # This block generates new samples by shifting the time steps of the data. \n",
    "        # The np.roll() function is used to shift the data along the time axis (axis=2). \n",
    "        # It shifts the entire time series by 1 or 2 time steps in both forward and backward directions (given by the shift values [-2, -1, 1, 2]).\n",
    "\t    # X_shifted = np.roll(X[i], shift, axis=2) shifts the time data in X[i] by shift units along the time axis.\n",
    "        '''\n",
    "        X[0] (shape: (3, 4)):\n",
    "        [[0.1, 0.2, 0.3, 0.4],\n",
    "         [0.5, 0.6, 0.7, 0.8],\n",
    "         [0.9, 1.0, 1.1, 1.2]]\n",
    "        \n",
    "        X_shifted = np.roll(X[0], 1, axis=2)\n",
    "        \n",
    "        X_shifted (shift = 1):\n",
    "        [[0.4, 0.1, 0.2, 0.3],\n",
    "         [0.8, 0.5, 0.6, 0.7],\n",
    "         [1.2, 0.9, 1.0, 1.1]]\n",
    "        '''\n",
    "\t    # The shifted data (X_shifted) is then appended to the augmented dataset (X_augmented), and the corresponding label (y[i]) is appended to y_augmented.\n",
    "        # Shifting the data with np.roll can potentially disrupt the chronological order of the data, and this could negatively affect the ability of the model to learn meaningful patterns from the data, especially when detecting a time-sensitive event like the N170.\n",
    "        for shift in [-2, -1, 1, 2]:  # Shift by 1 or 2 time steps\n",
    "            X_shifted = np.roll(X[i], shift, axis = 2)\n",
    "            X_augmented.append(X_shifted)\n",
    "            y_augmented.append(y[i])\n",
    "        \n",
    "        # Noise-injected data\n",
    "        # We add noise to the data to create new augmented samples that simulate real-world variations.\n",
    "\t    # noise = np.random.normal(0, 0.01, X[i].shape) generates random noise from a normal distribution with a mean of 0 and a standard deviation of 0.01. The noise has the same shape as the original data sample X[i].\n",
    "\t    # X_noisy = X[i] + noise adds the generated noise to the original sample.\n",
    "\t    # The noisy data (X_noisy) is then appended to X_augmented, and the corresponding label (y[i]) is appended to y_augmented.\n",
    "        noise = np.random.normal(0, 0.01, X[i].shape)\n",
    "        X_noisy = X[i] + noise\n",
    "        X_augmented.append(X_noisy)\n",
    "        y_augmented.append(y[i])\n",
    "\n",
    "    return np.array(X_augmented), np.array(y_augmented)\n",
    "\n",
    "# Apply augmentation on the whole dataset before splitting\n",
    "X_aug, y_aug = augment_data(X, y)\n",
    "\n",
    "print(f'Augmented data shape: X={X_aug.shape}, y={y_aug.shape}')\n",
    "\n",
    "X_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024.0\n",
      "[[[-4.85724608e-01]\n",
      "  [-5.27119373e-01]\n",
      "  [-5.62268219e-01]\n",
      "  [-5.90478106e-01]\n",
      "  [-6.11181882e-01]\n",
      "  [-6.23947065e-01]\n",
      "  [-6.28502286e-01]\n",
      "  [-6.24761993e-01]\n",
      "  [-6.12808235e-01]\n",
      "  [-5.92900675e-01]\n",
      "  [-5.65509937e-01]\n",
      "  [-5.31264647e-01]\n",
      "  [-4.90954150e-01]\n",
      "  [-4.45518758e-01]\n",
      "  [-3.96053484e-01]\n",
      "  [-3.43758174e-01]\n",
      "  [-2.89920554e-01]\n",
      "  [-2.35899617e-01]\n",
      "  [-1.83065431e-01]\n",
      "  [-1.32769500e-01]\n",
      "  [-8.62986058e-02]\n",
      "  [-4.48436433e-02]\n",
      "  [-9.45373979e-03]\n",
      "  [ 1.89875131e-02]\n",
      "  [ 3.98056557e-02]\n",
      "  [ 5.25734208e-02]\n",
      "  [ 5.71027349e-02]\n",
      "  [ 5.34707654e-02]\n",
      "  [ 4.19962167e-02]\n",
      "  [ 2.32894154e-02]\n",
      "  [-1.81074607e-03]\n",
      "  [-3.22203967e-02]\n",
      "  [-6.66553415e-02]\n",
      "  [-1.03667216e-01]\n",
      "  [-1.41677501e-01]\n",
      "  [-1.78994895e-01]\n",
      "  [-2.13913476e-01]\n",
      "  [-2.44730521e-01]\n",
      "  [-2.69807191e-01]\n",
      "  [-2.87654699e-01]\n",
      "  [-2.96989723e-01]\n",
      "  [-2.96749702e-01]\n",
      "  [-2.86137936e-01]\n",
      "  [-2.64685230e-01]\n",
      "  [-2.32264439e-01]\n",
      "  [-1.89059806e-01]\n",
      "  [-1.35538586e-01]\n",
      "  [-7.24695246e-02]\n",
      "  [-8.88158521e-04]\n",
      "  [ 7.79420526e-02]\n",
      "  [ 1.62560266e-01]\n",
      "  [ 2.51367286e-01]\n",
      "  [ 3.42658034e-01]\n",
      "  [ 4.34658349e-01]\n",
      "  [ 5.25540926e-01]\n",
      "  [ 6.13497115e-01]\n",
      "  [ 6.96732917e-01]\n",
      "  [ 7.73527571e-01]\n",
      "  [ 8.42280241e-01]\n",
      "  [ 9.01516806e-01]\n",
      "  [ 9.49977334e-01]\n",
      "  [ 9.86634087e-01]\n",
      "  [ 1.01072474e+00]\n",
      "  [ 1.02180830e+00]\n",
      "  [ 1.01978540e+00]\n",
      "  [ 1.00488770e+00]\n",
      "  [ 9.77711936e-01]\n",
      "  [ 9.39228262e-01]\n",
      "  [ 8.90681802e-01]\n",
      "  [ 8.33605057e-01]\n",
      "  [ 7.69750638e-01]\n",
      "  [ 7.01015686e-01]\n",
      "  [ 6.29382228e-01]\n",
      "  [ 5.56849028e-01]\n",
      "  [ 4.85371572e-01]\n",
      "  [ 4.16811412e-01]\n",
      "  [ 3.52875188e-01]\n",
      "  [ 2.95059875e-01]\n",
      "  [ 2.44652962e-01]\n",
      "  [ 2.02685261e-01]\n",
      "  [ 1.69931939e-01]\n",
      "  [ 1.46896147e-01]\n",
      "  [ 1.33773278e-01]\n",
      "  [ 1.30451800e-01]\n",
      "  [ 1.36548932e-01]\n",
      "  [ 1.51376120e-01]\n",
      "  [ 1.73985472e-01]\n",
      "  [ 2.03204857e-01]\n",
      "  [ 2.37699482e-01]\n",
      "  [ 2.75963647e-01]\n",
      "  [ 3.16408758e-01]\n",
      "  [ 3.57401396e-01]\n",
      "  [ 3.97329865e-01]\n",
      "  [ 4.34663860e-01]\n",
      "  [ 4.67979809e-01]\n",
      "  [ 4.96039020e-01]\n",
      "  [ 5.17826742e-01]\n",
      "  [ 5.32588458e-01]\n",
      "  [ 5.39823841e-01]\n",
      "  [ 5.39285499e-01]\n",
      "  [ 5.30985438e-01]\n",
      "  [ 5.15184104e-01]]\n",
      "\n",
      " [[-3.71863608e-01]\n",
      "  [-3.90977977e-01]\n",
      "  [-4.08156613e-01]\n",
      "  [-4.22955269e-01]\n",
      "  [-4.34996095e-01]\n",
      "  [-4.43945171e-01]\n",
      "  [-4.49559142e-01]\n",
      "  [-4.51673617e-01]\n",
      "  [-4.50229092e-01]\n",
      "  [-4.45290867e-01]\n",
      "  [-4.37021679e-01]\n",
      "  [-4.25674899e-01]\n",
      "  [-4.11624873e-01]\n",
      "  [-3.95336422e-01]\n",
      "  [-3.77347704e-01]\n",
      "  [-3.58273114e-01]\n",
      "  [-3.38766975e-01]\n",
      "  [-3.19511442e-01]\n",
      "  [-3.01180122e-01]\n",
      "  [-2.84418235e-01]\n",
      "  [-2.69827787e-01]\n",
      "  [-2.57956332e-01]\n",
      "  [-2.49246569e-01]\n",
      "  [-2.44053061e-01]\n",
      "  [-2.42604512e-01]\n",
      "  [-2.44982785e-01]\n",
      "  [-2.51126855e-01]\n",
      "  [-2.60866128e-01]\n",
      "  [-2.73877437e-01]\n",
      "  [-2.89696570e-01]\n",
      "  [-3.07728277e-01]\n",
      "  [-3.27273805e-01]\n",
      "  [-3.47549804e-01]\n",
      "  [-3.67718994e-01]\n",
      "  [-3.86887907e-01]\n",
      "  [-4.04146759e-01]\n",
      "  [-4.18606459e-01]\n",
      "  [-4.29398666e-01]\n",
      "  [-4.35728611e-01]\n",
      "  [-4.36867802e-01]\n",
      "  [-4.32192213e-01]\n",
      "  [-4.21221455e-01]\n",
      "  [-4.03617815e-01]\n",
      "  [-3.79205829e-01]\n",
      "  [-3.47961189e-01]\n",
      "  [-3.10025422e-01]\n",
      "  [-2.65698985e-01]\n",
      "  [-2.15435510e-01]\n",
      "  [-1.59837057e-01]\n",
      "  [-9.96469665e-02]\n",
      "  [-3.57191686e-02]\n",
      "  [ 3.09992995e-02]\n",
      "  [ 9.94696027e-02]\n",
      "  [ 1.68569254e-01]\n",
      "  [ 2.37137318e-01]\n",
      "  [ 3.03964919e-01]\n",
      "  [ 3.67836107e-01]\n",
      "  [ 4.27560169e-01]\n",
      "  [ 4.81970738e-01]\n",
      "  [ 5.29976528e-01]\n",
      "  [ 5.70576987e-01]\n",
      "  [ 6.02902180e-01]\n",
      "  [ 6.26243568e-01]\n",
      "  [ 6.40081236e-01]\n",
      "  [ 6.44100117e-01]\n",
      "  [ 6.38175879e-01]\n",
      "  [ 6.22426993e-01]\n",
      "  [ 5.97195496e-01]\n",
      "  [ 5.63052505e-01]\n",
      "  [ 5.20749743e-01]\n",
      "  [ 4.71215600e-01]\n",
      "  [ 4.15528881e-01]\n",
      "  [ 3.54861777e-01]\n",
      "  [ 2.90456578e-01]\n",
      "  [ 2.23589909e-01]\n",
      "  [ 1.55535660e-01]\n",
      "  [ 8.75447056e-02]\n",
      "  [ 2.08089205e-02]\n",
      "  [-4.35774183e-02]\n",
      "  [-1.04611634e-01]\n",
      "  [-1.61384393e-01]\n",
      "  [-2.13134514e-01]\n",
      "  [-2.59264368e-01]\n",
      "  [-2.99329085e-01]\n",
      "  [-3.33064896e-01]\n",
      "  [-3.60372745e-01]\n",
      "  [-3.81325057e-01]\n",
      "  [-3.96171102e-01]\n",
      "  [-4.05294016e-01]\n",
      "  [-4.09216790e-01]\n",
      "  [-4.08564498e-01]\n",
      "  [-4.04050803e-01]\n",
      "  [-3.96450940e-01]\n",
      "  [-3.86553263e-01]\n",
      "  [-3.75140224e-01]\n",
      "  [-3.62991298e-01]\n",
      "  [-3.50834067e-01]\n",
      "  [-3.39323552e-01]\n",
      "  [-3.29030284e-01]\n",
      "  [-3.20422161e-01]\n",
      "  [-3.13845304e-01]\n",
      "  [-3.09550355e-01]]\n",
      "\n",
      " [[-1.84946965e-01]\n",
      "  [-2.09305311e-01]\n",
      "  [-2.31217505e-01]\n",
      "  [-2.49863458e-01]\n",
      "  [-2.64492701e-01]\n",
      "  [-2.74405539e-01]\n",
      "  [-2.79018437e-01]\n",
      "  [-2.77877678e-01]\n",
      "  [-2.70681753e-01]\n",
      "  [-2.57304598e-01]\n",
      "  [-2.37793716e-01]\n",
      "  [-2.12365354e-01]\n",
      "  [-1.81429688e-01]\n",
      "  [-1.45556819e-01]\n",
      "  [-1.05456563e-01]\n",
      "  [-6.19560519e-02]\n",
      "  [-1.59677422e-02]\n",
      "  [ 3.15244853e-02]\n",
      "  [ 7.95093109e-02]\n",
      "  [ 1.26979226e-01]\n",
      "  [ 1.72952004e-01]\n",
      "  [ 2.16507920e-01]\n",
      "  [ 2.56807010e-01]\n",
      "  [ 2.93102565e-01]\n",
      "  [ 3.24783416e-01]\n",
      "  [ 3.51401384e-01]\n",
      "  [ 3.72690873e-01]\n",
      "  [ 3.88512676e-01]\n",
      "  [ 3.98919750e-01]\n",
      "  [ 4.04135753e-01]\n",
      "  [ 4.04546120e-01]\n",
      "  [ 4.00687594e-01]\n",
      "  [ 3.93212026e-01]\n",
      "  [ 3.82856633e-01]\n",
      "  [ 3.70442925e-01]\n",
      "  [ 3.56838303e-01]\n",
      "  [ 3.42927622e-01]\n",
      "  [ 3.29595045e-01]\n",
      "  [ 3.17683644e-01]\n",
      "  [ 3.08011128e-01]\n",
      "  [ 3.01322176e-01]\n",
      "  [ 2.98252681e-01]\n",
      "  [ 2.99339691e-01]\n",
      "  [ 3.04971293e-01]\n",
      "  [ 3.15391454e-01]\n",
      "  [ 3.30700845e-01]\n",
      "  [ 3.50835776e-01]\n",
      "  [ 3.75570779e-01]\n",
      "  [ 4.04521859e-01]\n",
      "  [ 4.37143519e-01]\n",
      "  [ 4.72779080e-01]\n",
      "  [ 5.10663521e-01]\n",
      "  [ 5.49940965e-01]\n",
      "  [ 5.89676168e-01]\n",
      "  [ 6.28882120e-01]\n",
      "  [ 6.66513344e-01]\n",
      "  [ 7.01520820e-01]\n",
      "  [ 7.32865679e-01]\n",
      "  [ 7.59513361e-01]\n",
      "  [ 7.80499830e-01]\n",
      "  [ 7.94923687e-01]\n",
      "  [ 8.01978396e-01]\n",
      "  [ 8.01009559e-01]\n",
      "  [ 7.91518513e-01]\n",
      "  [ 7.73195564e-01]\n",
      "  [ 7.45916143e-01]\n",
      "  [ 7.09775473e-01]\n",
      "  [ 6.65083862e-01]\n",
      "  [ 6.12379206e-01]\n",
      "  [ 5.52379792e-01]\n",
      "  [ 4.85985058e-01]\n",
      "  [ 4.14249998e-01]\n",
      "  [ 3.38333009e-01]\n",
      "  [ 2.59452034e-01]\n",
      "  [ 1.78866544e-01]\n",
      "  [ 9.78507787e-02]\n",
      "  [ 1.76533477e-02]\n",
      "  [-6.05413410e-02]\n",
      "  [-1.35628723e-01]\n",
      "  [-2.06589263e-01]\n",
      "  [-2.72516116e-01]\n",
      "  [-3.32658927e-01]\n",
      "  [-3.86433243e-01]\n",
      "  [-4.33407021e-01]\n",
      "  [-4.73342311e-01]\n",
      "  [-5.06176185e-01]\n",
      "  [-5.32013021e-01]\n",
      "  [-5.51121705e-01]\n",
      "  [-5.63921285e-01]\n",
      "  [-5.70948508e-01]\n",
      "  [-5.72827713e-01]\n",
      "  [-5.70259228e-01]\n",
      "  [-5.63989799e-01]\n",
      "  [-5.54778286e-01]\n",
      "  [-5.43375704e-01]\n",
      "  [-5.30515968e-01]\n",
      "  [-5.16875189e-01]\n",
      "  [-5.03068711e-01]\n",
      "  [-4.89628226e-01]\n",
      "  [-4.76970642e-01]\n",
      "  [-4.65398549e-01]\n",
      "  [-4.55114355e-01]]\n",
      "\n",
      " [[-7.98551477e-01]\n",
      "  [-8.79413593e-01]\n",
      "  [-9.55465766e-01]\n",
      "  [-1.02547320e+00]\n",
      "  [-1.08827473e+00]\n",
      "  [-1.14283150e+00]\n",
      "  [-1.18825880e+00]\n",
      "  [-1.22383873e+00]\n",
      "  [-1.24901522e+00]\n",
      "  [-1.26344075e+00]\n",
      "  [-1.26696867e+00]\n",
      "  [-1.25965661e+00]\n",
      "  [-1.24179878e+00]\n",
      "  [-1.21386491e+00]\n",
      "  [-1.17651180e+00]\n",
      "  [-1.13059406e+00]\n",
      "  [-1.07711426e+00]\n",
      "  [-1.01720015e+00]\n",
      "  [-9.52060113e-01]\n",
      "  [-8.82940550e-01]\n",
      "  [-8.11104941e-01]\n",
      "  [-7.37802751e-01]\n",
      "  [-6.64256763e-01]\n",
      "  [-5.91648935e-01]\n",
      "  [-5.21035211e-01]\n",
      "  [-4.53353148e-01]\n",
      "  [-3.89435048e-01]\n",
      "  [-3.29975071e-01]\n",
      "  [-2.75529981e-01]\n",
      "  [-2.26523750e-01]\n",
      "  [-1.83201919e-01]\n",
      "  [-1.45666920e-01]\n",
      "  [-1.13890128e-01]\n",
      "  [-8.76929753e-02]\n",
      "  [-6.67784109e-02]\n",
      "  [-5.07279321e-02]\n",
      "  [-3.89948588e-02]\n",
      "  [-3.09697935e-02]\n",
      "  [-2.59887719e-02]\n",
      "  [-2.33573981e-02]\n",
      "  [-2.23839333e-02]\n",
      "  [-2.24049276e-02]\n",
      "  [-2.27949784e-02]\n",
      "  [-2.29925722e-02]\n",
      "  [-2.25012576e-02]\n",
      "  [-2.09020814e-02]\n",
      "  [-1.78684243e-02]\n",
      "  [-1.31627000e-02]\n",
      "  [-6.63934800e-03]\n",
      "  [ 1.75321644e-03]\n",
      "  [ 1.19658890e-02]\n",
      "  [ 2.38850665e-02]\n",
      "  [ 3.73383374e-02]\n",
      "  [ 5.20795570e-02]\n",
      "  [ 6.78069178e-02]\n",
      "  [ 8.41887016e-02]\n",
      "  [ 1.00859552e-01]\n",
      "  [ 1.17434427e-01]\n",
      "  [ 1.33508212e-01]\n",
      "  [ 1.48676301e-01]\n",
      "  [ 1.62559611e-01]\n",
      "  [ 1.74782347e-01]\n",
      "  [ 1.84984349e-01]\n",
      "  [ 1.92857304e-01]\n",
      "  [ 1.98140782e-01]\n",
      "  [ 2.00610872e-01]\n",
      "  [ 2.00115401e-01]\n",
      "  [ 1.96601435e-01]\n",
      "  [ 1.90068820e-01]\n",
      "  [ 1.80575884e-01]\n",
      "  [ 1.68266701e-01]\n",
      "  [ 1.53346537e-01]\n",
      "  [ 1.36085874e-01]\n",
      "  [ 1.16799427e-01]\n",
      "  [ 9.58595685e-02]\n",
      "  [ 7.36535221e-02]\n",
      "  [ 5.05778732e-02]\n",
      "  [ 2.70437835e-02]\n",
      "  [ 3.50030978e-03]\n",
      "  [-1.96138705e-02]\n",
      "  [-4.18777219e-02]\n",
      "  [-6.28807525e-02]\n",
      "  [-8.22535360e-02]\n",
      "  [-9.96682006e-02]\n",
      "  [-1.14841006e-01]\n",
      "  [-1.27540333e-01]\n",
      "  [-1.37599889e-01]\n",
      "  [-1.44891938e-01]\n",
      "  [-1.49347182e-01]\n",
      "  [-1.50972937e-01]\n",
      "  [-1.49814088e-01]\n",
      "  [-1.45957828e-01]\n",
      "  [-1.39533510e-01]\n",
      "  [-1.30751673e-01]\n",
      "  [-1.19875929e-01]\n",
      "  [-1.07196781e-01]\n",
      "  [-9.30160692e-02]\n",
      "  [-7.76429442e-02]\n",
      "  [-6.14129186e-02]\n",
      "  [-4.46504534e-02]\n",
      "  [-2.76499732e-02]\n",
      "  [-1.07124948e-02]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36624, 4, 102, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_aug.shape[2]\n",
    "print(X_subject.info['sfreq'])\n",
    "\n",
    "X_aug.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.\tnb_classes\n",
    "\t•\tRepresents the number of output classes for the classification task.\n",
    "\t•\tThe number of output nodes in the final dense layer of your CNN\n",
    "\t•\tFor example, in binary classification (like your case of detecting two classes), nb_classes=2.\n",
    "\t\n",
    "    2.\tChans=4\n",
    "\t•\tThe number of input channels in the data (e.g., EEG channels such as P8, PO8, O2, P10).\n",
    "\t•\tThe architecture of the model uses this to define filter shapes (e.g., in the spatial convolution step).\n",
    "\t•\tIf you select 4 channels from the preprocessed data (['P8', 'PO8', 'O2', 'P10']), set Chans=4.\n",
    "\t\n",
    "    3.\tSamples=50\n",
    "\t•\tThe number of time samples in the input data.\n",
    "\t•\tHelps the model understand the temporal dimension of the signal.\n",
    "\t\n",
    "\t•\tIn your case, if each epoch spans from t=0.10 to t=0.20 seconds and is sampled at 500 Hz, you would have: Samples = (0.20 - 0.10) seconds * 1000 Hz = 100 samples.\n",
    "\t•\tThe sampling frequency (or sampling rate) of the EEG data is typically provided in the metadata of the dataset or is known beforehand based on the experimental setup used during the EEG recording.\n",
    "\t\n",
    "\t•\tSampling Frequency = Number of Samples / Time Duration\n",
    "\t•\tFrom your selected range t_min, t_max, which is 0.2 - 0.1 = 0.1 seconds. Sampling Frequency = 100 / 0.1 = 1000 Hz\n",
    "\t•\tX_subject.info['sfreq'] returns 1024 Hz and X.shape[2] returns 102.\n",
    "\n",
    "\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Overfitting occurs when a model learns the noise or specific details of the training data too well, and as a result, it performs poorly on unseen data (test data). Dropout is a regularization technique designed to prevent overfitting.\n",
    "\n",
    "By randomly deactivating (dropping out) a fraction of the neurons during training, dropout forces the network to learn more robust features. The model cannot rely on specific neurons too much, and this helps generalize better to unseen data.\n",
    "\n",
    "Why Use a Gradually Increasing Dropout Rate?\n",
    "\n",
    "\t1.\tEarly Layers: Retain Information\n",
    "\t•\tEarly layers of a neural network often learn low-level features (such as edges in images or simple patterns in time-series data).\n",
    "\t•\tIn these layers, dropout is typically lower (e.g., 0.2 or 0.3) because the network still needs to retain the important low-level features to build more complex representations in the later layers. Too much dropout here could hurt the network’s ability to learn these basic features.\n",
    "\t\n",
    "\t2.\tLater Layers: Prevent Overfitting\n",
    "\t•\tLater layers (e.g., the fully connected layers) tend to learn more abstract and complex features or combinations of lower-level features.\n",
    "\t•\tAs the model progresses, it becomes more powerful (i.e., more parameters), and hence the risk of overfitting increases. To counteract this, dropout is typically higher in later layers (e.g., 0.5), meaning 50% of the neurons are dropped. This helps prevent the model from relying too heavily on specific neurons and promotes better generalization.\n",
    "\t\n",
    "\t3.\tProgressive Regularization:\n",
    "\t•\tIncreasing the dropout rate as you move deeper into the network acts as a form of progressive regularization.\n",
    "\t•\tEarly in training, the network needs more capacity to learn the basic patterns, so a smaller dropout rate ensures it has sufficient power. However, once the basic features are learned, increasing the dropout in deeper layers reduces the chance that the model will memorize the specific details of the training data, helping to avoid overfitting.\n",
    "\n",
    "### Blocks\n",
    "\n",
    "1. block1 = Conv2D(25, (1, 5), padding = 'same', use_bias = False) (input_main)\n",
    "\n",
    "•\tThis is a 2D convolutional layer that learns spatial features from the input data.\n",
    "\n",
    "•\tConv2D is the 2D convolutional layer that is one of the building blocks of a Convolutional Neural Network (CNN). \n",
    "\n",
    "•\tThis layer performs convolutional operations on the input data to extract spatial features by sliding a filter (kernel) over the input. \n",
    "\n",
    "•\tIt is applied to 2D data (like images or time series data with multiple channels).\n",
    "\n",
    "•\tFilters (also called kernels) are learned during the training process. Each filter is responsible for detecting a specific pattern or feature in the input data (e.g., edges, temporal patterns in the case of EEG signals).\n",
    "\n",
    "#### EX\n",
    "\n",
    "•\t25: The number of filters or kernels(neurons for each layer) to apply in this convolutional layer. Each filter learns different features from the input, so this layer will output 25 feature maps. 25 means that 25 separate filters will be applied to the input data. This means that after applying this convolution, you will get 25 output feature maps (one for each filter). These feature maps will represent different learned features from the input data.\n",
    "\n",
    "•\t(1, 5): The size of the filter (kernel) used in the convolution. The filter has dimensions 1x5:\n",
    "\n",
    "•\t1 is the height of the filter, meaning it will span across the channels dimension (i.e., it’s looking at the entire channel).\n",
    "\n",
    "•\t5 is the width of the filter, meaning it spans across 5 time samples.\n",
    "\n",
    "•\tpadding='same': Ensures that the output feature map has the same width and height as the input feature map (i.e., padding is added to the input if necessary).\n",
    "\n",
    "•\tuse_bias=False: No bias is used in this layer.\n",
    "\n",
    "2. EEG-Specific Filters in CNNs:\n",
    "\n",
    "•\tFor EEG data, filters may span across time and channels, such as:\n",
    "\t•\t1 by 5: Detecting temporal patterns in a single channel.\n",
    "\t•\t4 by 1: Capturing spatial relationships across 4 channels.\n",
    "\t•\t4 by 5: Combining spatial and temporal information. \n",
    "\t\tex: # Block 1: Combining spatial and temporal information block1 = Conv2D(25, (Chans, 5), padding='same', use_bias=False)(input_main)  \n",
    "\n",
    "•\tinput_main vs. block1:\n",
    "\t•\tYou apply the first convolution directly on the input data (input_main).\n",
    "\t•\tFor subsequent layers, you apply the convolution on the output of the previous layer (e.g., block1).\n",
    "\n",
    "3. Temporal Filter for Input (Conv2D(25, (1, 5), padding='same')): Captures Temporal Features\n",
    "\n",
    "•\tTemporal: Detects frequency-specific patterns (e.g., alpha, beta, gamma bands).\n",
    "\n",
    "•\tCaptures patterns or relationships over time within each individual channel (electrode).\n",
    "\n",
    "•\tEEG signals are time-series data, so temporal filters help in detecting features like oscillations, event-related potentials (ERPs), or rapid changes in voltage.\n",
    "\n",
    "•\tBy applying a temporal filter of size (1, 5), we analyze small windows of time points for each channel independently.\n",
    "\n",
    "•\tHeight = 1: The height is fixed to 1 because the goal is to analyze each channel independently. By keeping the height as 1, the filter does not mix information and stay independent across channels.\n",
    "\n",
    "•\tWidth = 5: The width of 5 indicates the size of the temporal window being analyzed. This is a design choice and can vary based on:\n",
    "\t•\tThe sampling rate of your EEG data (e.g., how densely the time points are sampled).\n",
    "\t•\tThe temporal resolution of the patterns you’re trying to capture (e.g., short bursts vs. long trends).\n",
    "\n",
    "•\tFor example:\n",
    "\t•\tDetecting short-term changes in amplitude or frequency.\n",
    "\t•\tRecognizing local patterns like spikes or bursts in the signal.\n",
    "\n",
    "•\tThis step ensures the network extracts temporal dependencies in the data, which are crucial for identifying patterns like rhythmic \t\t activity (alpha waves, theta waves) or time-locked responses.\n",
    "\n",
    "4. Spatial Filter for Temporal Features (Conv2D(25, (Chans, 1))): Captures Spatial Features\n",
    "\n",
    "when the kernel is applied, the result is placed at the top-left corner (or the position where the kernel is centered)\n",
    "\n",
    "•\tSpatial: Identifies brain regions generating or interacting with these oscillations.\n",
    "\n",
    "•\tAfter extracting temporal patterns within each channel, we apply a spatial filter to combine information across channels.\n",
    "\n",
    "•\tCombines information across multiple channels (electrodes).\n",
    "\n",
    "•\tCaptures the spatial distribution of activity, which can reveal interactions or correlations between different parts of the brain.\n",
    "\n",
    "•\tEEG signals recorded at different electrodes (channels) are not independent—they often exhibit spatial dependencies:\n",
    "\t•\tFor instance, responses in occipital electrodes (back of the head) might correlate with visual stimuli.\n",
    "\t•\tSpatial filters help detect distributed patterns of activation or relationships between electrodes.\n",
    "\t•\tIn some cases, specific brain activities (e.g., N170 ERP) may appear localized in certain regions but still involve coordinated activity across multiple electrodes.\n",
    "\n",
    "•\tBy applying a filter of size (Chans, 1), the network learns how different channels contribute together to meaningful patterns.\n",
    "\n",
    "#### EX for X_aug.shape = (36624, 4, 102, 1)\n",
    "\n",
    "Where:\n",
    "\t•\t36624 is the batch size (number of samples).\n",
    "\t•\t4 is the number of channels (EEG electrodes).\n",
    "\t•\t102 is the number of time samples (time points).\n",
    "\t•\t1 is the depth (single-channel data).\n",
    "\n",
    "##### Step 1: Temporal Convolution (Conv2D(25, (1, 5), padding='same', use_bias=False)(input_main))\n",
    "\n",
    "This layer applies a temporal filter of size (1, 5) across the time dimension (102 time points) for each channel independently. The filter has:\n",
    "\t•\tA height of 1 (which means it’s applied across all the channels), and\n",
    "\t•\tA width of 5 (which means it’s applied across 5 consecutive time points).\n",
    "\n",
    "Since you’re using padding='same', the output width will remain the same as the input width (102), and the depth increases to 25 (because you are using 25 filters). The height remains 4 since the filter only operates along the time dimension, and the channels are not affected.\n",
    "\n",
    "After this operation, the output shape will be: (36624, 4, 102, 25)\n",
    "\n",
    "\t•\tBatch size: 36624 (unchanged).\n",
    "\t•\tChannels: 4 (unchanged).\n",
    "\t•\tTime points (Samples): 102 (unchanged, because of padding='same').\n",
    "\t•\tDepth (Filters): 25 (as specified by the number of filters).\n",
    "\n",
    "So, the neurons in the first layer will have the shape (4, 102, 25). This means for each time point (in the time dimension) and each EEG channel (in the channel dimension), there will be 25 different features (neurons) created by the 25 filters.\n",
    "\n",
    "##### Step 2: Spatial Convolution (Conv2D(25, (Chans, 1), use_bias=False)(block1))\n",
    "\n",
    "This second convolution layer applies a spatial filter of size (4, 1) (because Chans = 4) across the channel dimension (4 EEG channels). The filter has:\n",
    "\t•\tA height of 4 (which means it’s applied across all 4 channels), and\n",
    "\t•\tA width of 1 (which means it’s applied across a single time point).\n",
    "\n",
    "Since you are using padding='same', the width (time samples) remains unchanged at 102, but the height (channels) will reduce to 1 because the filter aggregates information across all channels into a single spatial representation for each time point.\n",
    "\n",
    "After this operation, the output shape will be: (36624, 1, 102, 25)\n",
    "\n",
    "\t•\tBatch size: 36624 (unchanged).\n",
    "\t•\tChannels: 1 (because the filter aggregates all channels into one).\n",
    "\t•\tTime points (Samples): 102 (unchanged).\n",
    "\t•\tDepth (Filters): 25 (as specified by the number of filters).\n",
    "\n",
    "\n",
    "5. Why Perform These Steps Sequentially?\n",
    "\n",
    "•\tTemporal first, then spatial:\n",
    "\n",
    "•\tEEG signals are first processed channel-wise because the temporal dynamics within a channel are critical and often unique to that channel.\n",
    "\n",
    "•\tOnce these temporal features are extracted, combining them across channels enables the network to learn global spatiotemporal patterns that span the entire brain region.\n",
    "\n",
    "•\tAvoid losing temporal resolution early:\n",
    "•\tIf we combined channels first (spatial filtering) before applying temporal filters, we would lose detailed temporal information about each channel’s signal.\n",
    "\n",
    "6. Why Not Apply Both Simultaneously?\n",
    "\n",
    "It’s possible to design filters that analyze both temporal and spatial dimensions at the same time (e.g., (Chans, 5)). However:\n",
    "•\tDecoupling spatial and temporal processing simplifies training because:\n",
    "•\tTemporal filters only learn from time-domain relationships (fewer parameters to optimize).\n",
    "•\tSpatial filters only learn from spatial relationships (also fewer parameters).\n",
    "\n",
    "•\tSequential processing is particularly useful for EEG data, where:\n",
    "•\tTemporal patterns (oscillations, bursts) are often more consistent across trials, while spatial patterns can vary.\n",
    "•\tIsolating the two dimensions helps the network focus on distinct aspects of the data.\n",
    "\n",
    "7. What happens with padding=‘same’ in convolution:\n",
    "\n",
    "(Chans, samples, depth)\n",
    "\n",
    "•\tPadding = ‘same’ means that the spatial dimensions (height and width) of the input and output will remain the same if the stride is 1.\n",
    "\n",
    "•\tSpatial dimension (height): If you’re using padding='same', the height of the input will stay the same in the output (if the stride is 1). In your case, this is the number of EEG channels, which is fixed.\n",
    "\n",
    "•\tTemporal dimension (width): The width of the input (number of time samples) will also stay the same because of the same padding, assuming stride is 1. This corresponds to the number of time points.\n",
    "\n",
    "•\tDepth: The number of feature maps or filters used in the previous layer\n",
    "•\tFirst Convolution (block1): After applying the convolution with the filter size (1, 5), the depth will be 25 (as specified).\n",
    "•\tSecond Convolution (block2): After applying the next convolution with the filter size (1, 5), the depth increases to 50 because you specified 50 filters in the layer.\n",
    "\n",
    "8. What is Bias in a Neural Network?\n",
    "\n",
    "In a typical neural network, each layer has two components that influence the output:\n",
    "\t1.\tWeights: These define the strength of the connections between the neurons (or, in the case of convolutional layers, the kernel/filter).\n",
    "\t2.\tBias: This is an additional parameter added to the output of the convolution operation before applying the activation function. It shifts the activation of the neuron and can help the network learn a wider range of functions.\n",
    "\n",
    "In EEG, the temporal and spatial dependencies between different channels and time points are critical. By using use_bias=False in conjunction with batch normalization, you are allowing the model to focus more on these dependencies without introducing unnecessary complexity. In fact, when dealing with EEG, the model needs to:\n",
    "•\tLearn temporal patterns (how EEG signals evolve over time),\n",
    "•\tLearn spatial patterns (how different channels might correlate or react together to certain stimuli, like faces),\n",
    "•\tAdapt to these patterns through convolutional filters and batch normalization (which controls variance and scaling).\n",
    "\n",
    "9. Batch Normalization (block1 = BatchNormalization()(block1))\n",
    "\n",
    "Batch normalization is applied to the feature maps generated by the previous convolutional layer. Here’s what it does:\n",
    "\n",
    "It normalizes the activations of the previous layer across the mini-batch (across the batch dimension). This helps in reducing internal covariate shift, where the distribution of activations changes during training, which can slow down training and make it unstable.\n",
    "\n",
    "\n",
    "\n",
    "Activations are the values produced by neurons in a layer after the network processes the input data and applies an activation function to them. \n",
    "\n",
    "For example, in a fully connected layer, the activation might be calculated as a weighted sum of the inputs to the neuron, followed by the application of a non-linear function like ReLU, Sigmoid, or ELU (feature map in out case).\n",
    "•\tActivation = sigma(W * X + b) \n",
    "\n",
    "\n",
    "\n",
    "In the context of batch normalization, the activation changes refer to the modification of these outputs across the network’s layers, particularly as they propagate forward during training.\n",
    "\n",
    "Batch normalization changes the activations of neurons across the network in a specific way:\n",
    "\n",
    "•\tFor each mini-batch of data, the activations of each neuron in that mini-batch are calculated.\n",
    "\n",
    "•\tThe mean and standard deviation of the activations for each feature map (i.e., each channel or neuron) are computed across all examples in the mini-batch.\n",
    "\n",
    "•\tThe activations are then normalized by subtracting the mean and dividing by the standard deviation. This process makes the activations have zero mean and unit variance, reducing internal covariate shift, which helps with model stability and speed during training.\n",
    "\n",
    "\n",
    "\n",
    "The goal of normalizing activations is to reduce the variability in the distribution of activations from one layer to the next. \n",
    "\n",
    "Without normalization, the distribution of activations can vary significantly during training, which can cause problems for training stability and convergence speed (this is called internal covariate shift).\n",
    "\t\n",
    "•\tInternal covariate shift occurs when the distribution of outputs from one layer shifts during training as the network weights are updated, which can make it harder for the next layer to learn.\n",
    "\t\n",
    "•\tBatch normalization reduces this issue by ensuring that activations have a stable distribution across layers.\n",
    "\n",
    "•\tDuring training, the model processes the data in mini-batches rather than using the entire dataset at once (which is called batch gradient descent) or one data point at a time (which is stochastic gradient descent).\n",
    "\n",
    "•\tA mini-batch is a subset of the training data that the model processes simultaneously. For example, if the batch size is 32, each mini-batch will consist of 32 randomly selected samples from the training dataset.\n",
    "\n",
    "•\tBatch Normalization is applied independently at each layer. When you have mini-batches of size 32 (for example), each of the layers (block1, block2, block3, block4) will receive and normalize the data in mini-batches of 32 samples, and each layer will compute the normalization using the batch it is processing at that time. \n",
    "\n",
    "•\tThe epochs for the mini-batch selection are chosen from the original data: normalizing the feature data at each layer based on the mini-batch from the original data, not the processed feature maps\n",
    "\n",
    "•\tThe feature maps for each neuron are normalized by subtracting the mini-batch mean and dividing by the mini-batch standard deviation. This helps ensure that the activations from different neurons in the layer are in a similar range, improving training stability.\n",
    "\n",
    "10. ELU (Exponential Linear Unit)\n",
    "\n",
    "Activation functions like ELU, ReLU, Sigmoid, and others are essential components of neural networks because they introduce non-linearity into the model. Here’s why we apply these functions to the output of neurons in a neural network:\n",
    "\n",
    "activation functions help “normalize” or “scale” the input values to a range that’s easier for the network to work with\n",
    "\n",
    "Introducing Non-Linearity:\n",
    "\n",
    "•\tNeural networks aim to model complex relationships in data. Without activation functions, a neural network would essentially be just a linear model, no matter how many layers it had. This is because stacking multiple linear operations (like matrix multiplications) still results in a linear transformation of the input.\n",
    "\n",
    "•\tNon-linear activation functions allow the network to model more complex patterns. They enable the network to learn and approximate complex functions or decision boundaries that would be impossible with just linear transformations.\n",
    "\n",
    "•\tIn short, without these activation functions, even a deep neural network would behave like a single-layer model, unable to capture the intricacies of complex data like EEG signals.\n",
    "\n",
    "ELU = \n",
    "x, if x > 0 \n",
    "alpha (exp(x) - 1), if x \n",
    "\n",
    "•\tELU behaves like ReLU for positive values (outputs the input directly).\n",
    "\n",
    "•\tFor negative values, it outputs an exponentially decaying value, which helps to prevent neurons from being “dead” (always outputting zero like in ReLU).\n",
    "\n",
    "•\tBoth tanh and sigmoid suffer from the vanishing gradient problem when the input values are very large or very small. This is because their derivatives approach zero at the extremes of their input range.\n",
    "\n",
    "•\tELU, on the other hand, avoids this issue for positive inputs (since its gradient is constant for positive values) and smoothens the behavior for negative inputs, making it a good choice for deep networks.\n",
    "\n",
    "•\tIn your EEG project, particularly with N170 detection, the data likely contains negative values (as EEG signals fluctuate around a baseline), so using ReLU could cause neurons to “die” and stop contributing to learning if they encounter negative values.\n",
    "\n",
    "•\tELU ensures that both negative and positive activations are treated properly, and the network learns better from both types of input.\n",
    "\n",
    "•\tELU also helps speed up training, which is important when dealing with large datasets like EEG signals with many epochs.\n",
    "\n",
    "11. max pooling operation\n",
    "\n",
    "MaxPooling is a downsampling operation commonly used in convolutional neural networks (CNNs). \n",
    "\n",
    "It reduces the spatial dimensions (height and width) of the input while retaining the most important information. \n",
    "\n",
    "This helps to reduce the number of parameters, computational load, and the risk of overfitting.\n",
    "\n",
    "The MaxPooling2D layer operates over a 2D window (or filter) that slides over the input.\n",
    "\n",
    "The window size is defined by the tuple (1, 2). This means that:\n",
    "\n",
    "\tAlong the height (time dimension), the window moves by 1 unit (no change in the height dimension).\n",
    "\n",
    "\tAlong the width (sample dimension), the window moves by 2 units at a time, reducing the width by a factor of 2.\n",
    "\n",
    "For each window, the maximum value within the window is selected and passed to the next layer.\n",
    "\n",
    "##### Ex. Applying MaxPooling2D((1, 2)):\n",
    "\n",
    "•\tThe window size is (1, 2), which means the pooling operation will look at 2 samples at a time (along the columns) and pick the maximum value for each row across these two adjacent samples. The height (time dimension) remains the same because the window size along the height is 1.\n",
    "\n",
    "Time 1 (a1,1)\tTime 2 (a1,2)\tTime 3 (a1,3)\tTime 4 (a1,4)\tTime 5 (a1,5)\tTime 6 (a1,6)\tTime 7 (a1,7)\tTime 8 (a1,8)\n",
    "a1,1\t\t\ta1,2\t\t\ta1,3\t\t\ta1,4\t\t\ta1,5\t\t\ta1,6\t\t\ta1,7\t\t\ta1,8\n",
    "a2,1\t\t\ta2,2\t\t\ta2,3\t\t\ta2,4\t\t\ta2,5\t\t\ta2,6\t\t\ta2,7\t\t\ta2,8\n",
    "a3,1\t\t\ta3,2\t\t\ta3,3\t\t\ta3,4\t\t\ta3,5\t\t\ta3,6\t\t\ta3,7\t\t\ta3,8\n",
    "a4,1\t\t\ta4,2\t\t\ta4,3\t\t\ta4,4\t\t\ta4,5\t\t\ta4,6\t\t\ta4,7\t\t\ta4,8\n",
    "\n",
    "\t1.\tFirst window (samples 1 and 2):\n",
    "\t\t•\tLook at columns 1 and 2:\n",
    "\t\t•\tMax(a1,1, a1,2) = max(a1,1, a1,2)\n",
    "\t\t•\tMax(a2,1, a2,2) = max(a2,1, a2,2)\n",
    "\t\t•\tMax(a3,1, a3,2) = max(a3,1, a3,2)\n",
    "\t\t•\tMax(a4,1, a4,2) = max(a4,1, a4,2)\n",
    "\t\t•\tSo, the pooled output for the first window is the maximum value from columns 1 and 2 for each row.\n",
    "\t\n",
    "\t2.\tSecond window (samples 3 and 4):\n",
    "\t•\tLook at columns 3 and 4:\n",
    "\t•\tMax(a1,3, a1,4) = max(a1,3, a1,4)\n",
    "\t•\tMax(a2,3, a2,4) = max(a2,3, a2,4)\n",
    "\t•\tMax(a3,3, a3,4) = max(a3,3, a3,4)\n",
    "\t•\tMax(a4,3, a4,4) = max(a4,3, a4,4)\n",
    "\t\n",
    "\t3.\tThird window (samples 5 and 6):\n",
    "\t•\tLook at columns 5 and 6:\n",
    "\t•\tMax(a1,5, a1,6) = max(a1,5, a1,6)\n",
    "\t•\tMax(a2,5, a2,6) = max(a2,5, a2,6)\n",
    "\t•\tMax(a3,5, a3,6) = max(a3,5, a3,6)\n",
    "\t•\tMax(a4,5, a4,6) = max(a4,5, a4,6)\n",
    "\t\n",
    "\t4.\tFourth window (samples 7 and 8):\n",
    "\t•\tLook at columns 7 and 8:\n",
    "\t•\tMax(a1,7, a1,8) = max(a1,7, a1,8)\n",
    "\t•\tMax(a2,7, a2,8) = max(a2,7, a2,8)\n",
    "\t•\tMax(a3,7, a3,8) = max(a3,7, a3,8)\n",
    "\t•\tMax(a4,7, a4,8) = max(a4,7, a4,8)\n",
    "\n",
    "Time 1 (a1,1)\tTime 2 (a1,3)\tTime 3 (a1,5)\tTime 4 (a1,7)\n",
    "max(a1,1, a1,2)\tmax(a1,3, a1,4)\tmax(a1,5, a1,6)\tmax(a1,7, a1,8)\n",
    "max(a2,1, a2,2)\tmax(a2,3, a2,4)\tmax(a2,5, a2,6)\tmax(a2,7, a2,8)\n",
    "max(a3,1, a3,2)\tmax(a3,3, a3,4)\tmax(a3,5, a3,6)\tmax(a3,7, a3,8)\n",
    "max(a4,1, a4,2)\tmax(a4,3, a4,4)\tmax(a4,5, a4,6)\tmax(a4,7, a4,8)\n",
    "\n",
    "12. What is Dropout?\n",
    "\n",
    "•\tA base dropout rate for regularization.\n",
    "•\tSpecifies the fraction of neurons to randomly “turn off” during training to prevent overfitting.\n",
    "\n",
    "•\tDuring training, 50% of the neurons in the layer will be randomly deactivated (or “dropped out”) at each training step\n",
    "•\tThis helps prevent the network from relying too heavily on any single neuron, promoting more robust learning.\n",
    "\n",
    "•\t0 means no dropout (i.e., all neurons are active during training).\n",
    "•\t1 means all neurons are deactivated, which is not practical because the network would not learn.\n",
    "•\t0.2 (for early layers)\n",
    "•\t0.3 to 0.4 (for deeper layers)\n",
    "•\t0.5 (for the final fully connected layers to prevent overfitting)\n",
    "\n",
    "•\tRecommendations:\n",
    "•\t0.2 to 0.3: For shallow networks or when you want to allow more information flow through the network.\n",
    "•\t0.4 to 0.5: For deeper networks or more complex models to ensure strong regularization and prevent overfitting.\n",
    "\n",
    "•\tIn your case, in the code you shared, different dropout rates are applied to different layers:\n",
    "•\tBlock 1: Dropout rate is 0.2. So, 20% of the neurons in this layer will be randomly deactivated.\n",
    "•\tBlock 2: Dropout rate is 0.3. 30% of the neurons in this layer will be dropped.\n",
    "•\tBlock 3: Dropout rate is 0.4. 40% of the neurons in this layer will be dropped.\n",
    "•\tBlock 4: Dropout rate is 0.5. 50% of the neurons in this layer will be dropped.\n",
    "\n",
    "\n",
    "\n",
    "How does Dropout work?\n",
    "\n",
    "•\tDuring training:\n",
    "\n",
    "•\tDropout will randomly set 20% of the activations in the input tensor to zero.\n",
    "•\tThe remaining 80% of neurons will be scaled up to ensure that the overall output is balanced. This means the activations of the remaining neurons are multiplied by a factor of 1/(1 - 0.2) = 1.25 during training.\n",
    "\n",
    "\n",
    "•\tDuring inference (testing or prediction):\n",
    "\n",
    "•\tNo dropout is applied. All neurons are used for computation, but the weights are scaled down by the same factor of 0.8 to match the effect of training.\n",
    "\n",
    "\n",
    "\n",
    "13. Why Dropout?\n",
    "\n",
    "•\tPrevents overfitting: Dropout forces the model to not rely on specific neurons and helps it generalize better to unseen data.\n",
    "\n",
    "•\tImproves robustness: By randomly disabling neurons during training, the network learns to create more robust features and can generalize to new patterns more effectively.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "Imagine that block1 is a feature map with shape (36624, 1, 102, 25) (for simplicity, let’s assume it’s just 25 channels and 102 time points for a single epoch). When Dropout is applied:\n",
    "\n",
    "•\tRandomly 20% of the values from the block1 output will be set to zero.\n",
    "\n",
    "•\tThis means that during each training iteration, the network will “forget” about certain neurons (features) and only focus on the remaining 80%.\n",
    "\n",
    "\n",
    "Increasing the Number of Filters:\n",
    "\n",
    "•\tPurpose: The number of filters (also known as channels or feature maps) increases as we go deeper into the network because deeper layers are meant to learn increasingly abstract and complex features from the data.\n",
    "\n",
    "•\tReason: Early layers of the network (e.g., Block 1) focus on simple features like edges, textures, or local patterns. As the network progresses, it captures more complex, high-level features that are built from the simpler ones detected earlier.\n",
    "\n",
    "•\tFor example:\n",
    "•\tBlock 1 (25 filters) might learn simple patterns in the data (like edges or small features in EEG signals).\n",
    "•\tBlock 2 (50 filters) might combine these simple features to detect more complex patterns.\n",
    "•\tBlock 3 (100 filters) could detect even higher-level patterns, such as combinations of previous patterns.\n",
    "•\tBlock 4 (200 filters) can capture even more intricate patterns or more abstract representations.\n",
    "\n",
    "By increasing the number of filters, the network has the capacity to learn more complex features from the data. This is especially useful as the layers get deeper, and the network needs more “capacity” to model more abstract representations.\n",
    "\n",
    "14. Increasing the Dropout Rate:\n",
    "\n",
    "•\tPurpose: Dropout is a regularization technique that helps prevent overfitting by randomly “dropping out” (setting to zero) a fraction of the neurons in the network during training. The purpose of increasing the dropout rate as you go deeper in the network is to reduce overfitting and improve generalization as the model learns more complex patterns.\n",
    "\n",
    "•\tReason:\n",
    "\n",
    "•\tIn the early layers (Block 1), the network is learning basic features, so overfitting is less of an issue. A lower dropout rate (e.g., 0.2) allows the model to focus on learning the simpler features effectively.\n",
    "\n",
    "•\tAs the network gets deeper, the features it learns become more complex. At this point, overfitting to these more complex features can become a greater risk. Increasing the dropout rate (e.g., to 0.5 in Block 4) forces the network to regularize more heavily and prevents it from relying too much on any single neuron, which helps improve generalization to unseen data.\n",
    "\n",
    "14. Overview of Training Phases\n",
    "\n",
    "\t1.\tBlock 1 to Block 4: Feature Extraction\n",
    "\t•\tThe layers in Block 1 to Block 4 are primarily focused on feature extraction. These blocks consist of convolutional layers (Conv2D), activation functions (e.g., ELU), normalization (BatchNormalization), pooling layers (MaxPooling2D), and dropout layers (Dropout).\n",
    "\t•\tThe role of these layers is to process the input data, extract important features, and reduce the spatial dimensions while increasing the depth (number of filters).\n",
    "\t•\tDuring training, these layers are trained through backpropagation, meaning the weights in the convolutional layers (filters) are adjusted to minimize the loss function.\n",
    "\t\n",
    "\t2.\tFlattening the Output\n",
    "\t•\tAfter Block 4, you have a feature map with 1 row and 1 column (if we assume this scenario). This output is passed through a Flatten layer, which converts the multi-dimensional feature map into a 1D vector.\n",
    "\t•\tThis flattened vector is what will be passed to the Dense layer.\n",
    "\t\n",
    "\t3.\tDense Layer: Final Classification (Fully Connected Layer)\n",
    "\t•\tThe Dense layer is a fully connected layer, meaning it connects every input neuron (from the flattened feature map) to every output neuron (nb_classes, in this case).\n",
    "\t•\tIn the Dense layer, the network learns to map the features extracted by the previous blocks to the final classification output (e.g., class probabilities).\n",
    "\t\n",
    "\t4.\tActivation (Softmax)\n",
    "\t•\tThe output from the Dense layer is passed through a Softmax activation. Softmax is typically used in classification tasks to convert the output scores into probability distributions over the classes (for multi-class classification). It ensures that the sum of the output values is 1 and each value represents the likelihood of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN Model Implementation using DeepConvNet\n",
    "# Defines a convolutional neural network (CNN) tailored for classification tasks.\n",
    "\n",
    "def DeepConvNet(nb_classes, Chans, Samples):\n",
    "    \n",
    "    # This defines the shape of the input data that the model will receive.\n",
    "    # 1: Indicates that the input is a single-channel input (i.e., grayscale or a single feature per time sample)\n",
    "    input_main = Input((Chans, Samples, 1))\n",
    "    \n",
    "    # Block 1\n",
    "    block1 = Conv2D(25, (1, 5), padding = 'same', use_bias = False) (input_main)\n",
    "    block1 = Conv2D(25, (Chans, 1), use_bias = False) (block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    # applying the ELU (Exponential Linear Unit) activation function to the output of the previous layer\n",
    "    block1 = Activation('elu')(block1)\n",
    "    block1 = MaxPooling2D((1, 2))(block1)\n",
    "    # Hard coded dropout rate instead of using a parameter for the progessive regularization\n",
    "    block1 = Dropout(0.2)(block1)\n",
    "    \n",
    "    # Block 2\n",
    "    block2 = Conv2D(50, (1, 5), padding='same', use_bias=False)(block1)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = MaxPooling2D((1, 2))(block2)\n",
    "    block2 = Dropout(0.3)(block2)\n",
    "    \n",
    "    # Block 3\n",
    "    block3 = Conv2D(100, (1, 5), padding='same', use_bias=False)(block2)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = Activation('elu')(block3)\n",
    "    block3 = MaxPooling2D((1, 2))(block3)\n",
    "    block3 = Dropout(0.4)(block3)\n",
    "    \n",
    "    # Block 4\n",
    "    block4 = Conv2D(200, (1, 5), padding='same', use_bias=False)(block3)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = Activation('elu')(block4)\n",
    "    block4 = MaxPooling2D((1, 2))(block4)\n",
    "    block4 = Dropout(0.5)(block4)\n",
    "    \n",
    "    ### Flatten and Dense Layers\n",
    "    # After your convolutional and pooling layers, the feature map size has been reduced to 1 \\times 1 \\times N, where N is the number of channels (depth). \n",
    "    # The Flatten() layer is used to convert this 3D tensor into a 1D vector (flattened vector) so that it can be fed into fully connected (dense) layers.\n",
    "    '''\n",
    "    1.\tInput to Flatten:\n",
    "\t•\tSuppose the output from Block 4 is a feature map of size (1, 8, 200). This means it has 1 row, 8 columns, and 200 channels (depth). These 200 channels represent the feature maps learned by Block 4.\n",
    "\t2.\tFlattening Process:\n",
    "\t•\tThe Flatten operation takes the entire (1, 8, 200) feature map and converts it into a 1D vector.\n",
    "\t•\tThe size of the vector will be the product of the dimensions of the feature map. So, if the feature map is (1, 8, 200), \n",
    "        flattening it will result in a vector of size: 1 * 8 * 200 = 1600\n",
    "\t•\tThis means the output of the Flatten operation will be a 1D vector of size 1600.\n",
    "    '''\n",
    "    flatten = Flatten()(block4)\n",
    "\t# The Dense layer (also called a fully connected layer) applies a set of weights to the input vector. Each neuron in the Dense layer is connected to every value in the input vector.\n",
    "\t# nb_classes represents the number of output units or neurons in the dense layer, which typically corresponds to the number of categories (or classes) you want to classify.\n",
    "    # For example, if nb_classes = 10, this means that the Dense layer will output a vector of size 10.\n",
    "\t# In a Dense layer, the input vector (of size 1600, in our case) is multiplied by the weight matrix of the layer and then has a bias term added to it.\n",
    "    # If nb_classes = 2 and the flatten is a 1D vector with 1600 values, the Dense layer will output a 1D vector of size 2, which represents the scores for the two classes.\n",
    "    # W = [[w1,1, w1,2, ..., w1,1600],  is multiplied by 1 by 1600 input vector x\n",
    "    #      [w2,1, w2,2, ..., w2,1600]]\n",
    "    dense = Dense(nb_classes)(flatten)\n",
    "    # After the Dense layer, you apply an activation function (like Softmax or Sigmoid) to get probabilities for each class, which helps in classification tasks.\n",
    "    # Softmax: Usually used when you want to treat the outputs as mutually exclusive classes (one class is chosen).\n",
    "\t# Example: Classifying an image into one of 10 categories (e.g., cat, dog, bird, etc.).\n",
    "\t# Sigmoid: Used for binary decisions or when multiple classes can be true simultaneously (multi-label classification).\n",
    "\t# Example: Classifying an image as containing a cat and a dog (both can be true at the same time).\n",
    "    softmax = Activation('softmax')(dense)\n",
    "    \n",
    "    return Model(inputs = input_main, outputs = softmax)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gradient Descent and Local Minima\n",
    "\n",
    "What is Gradient Descent?\n",
    "\n",
    "\t•\tGradient Descent is an optimization algorithm used to minimize a loss function (a mathematical measure of how far the network’s predictions are from the actual labels).\n",
    "\t\n",
    "    •\tThe gradient of the loss function with respect to the weights indicates the direction and magnitude of change required to reduce the loss.\n",
    "\n",
    "How Does It Work?\n",
    "\n",
    "\t1.\tForward Pass:\n",
    "\t•\tInput data is passed through the network layers (convolutions, activations, pooling, etc.), producing an output (prediction).\n",
    "\t\n",
    "    2.\tCompute Loss:\n",
    "\t•\tThe output is compared to the ground truth (actual labels) using a loss function (e.g., cross-entropy for classification).\n",
    "\t\n",
    "    3.\tBackward Pass (Backpropagation):\n",
    "\t•\tThe gradient of the loss with respect to each weight in the network is computed using the chain rule.\n",
    "\t•\tGradients flow backward from the output layer to earlier layers, updating the weights to reduce the loss.\n",
    "\n",
    "Key Role of Local Minima:\n",
    "\n",
    "\t•\tBy adjusting weights based on the gradient, the algorithm moves toward a local minimum of the loss function, where the loss is lowest and the predictions are most accurate.\n",
    "\t\n",
    "    •\tGlobal vs. Local Minima: In deep learning, the loss landscape is highly complex with many local minima. Modern optimization techniques (like stochastic gradient descent) help navigate this landscape effectively.\n",
    "\n",
    "2. Gradient Calculation\n",
    "\n",
    "\t•\tFor each training sample or batch:\n",
    "\t•\tDuring the forward pass, the model predicts an output using the current weights.\n",
    "\t•\tThe loss is calculated by comparing the predicted output to the true label.\n",
    "\t•\tDuring the backward pass, gradients of the loss with respect to each weight (∂Loss/∂W) are computed. These gradients represent the direction and magnitude of change needed to minimize the loss.\n",
    "\n",
    "\tFor each weight  W_{ij} :\n",
    "\t•\tGradients are calculated independently for that weight.\n",
    "\t•\tThe weight is updated using the gradient:\n",
    "\n",
    "\tW_{ij} \\leftarrow W_{ij} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W_{ij}}\n",
    "\n",
    "\tWhere:\n",
    "\t\t•\t \\eta  is the learning rate (a small positive number determining the step size).\n",
    "\t\t•\t \\frac{\\partial \\text{Loss}}{\\partial W_{ij}}  is the gradient of the loss with respect to  W_{ij} .\n",
    "\t\n",
    "\tWhen training in batches or mini-batches:\n",
    "\t•\tGradients are computed for each sample in the batch.\n",
    "\t•\tThe gradients are then averaged across the batch.\n",
    "\t•\tThe averaged gradient is used to update the weights.\n",
    "\n",
    "\tThis averaging ensures that the updates are more stable and reflect the overall trend of the batch rather than being influenced by individual noisy samples.\n",
    "\n",
    "3.\tOptimize the Cost Function using Derivatives\n",
    "\n",
    "Derivatives quantify how small changes in one variable (e.g., a weight or bias) affect another variable (e.g., the cost). \n",
    "\n",
    "\t•\tA derivative measures the rate of change of one variable with respect to another.\n",
    "\t•\tFor a function  f(x) , the derivative  f{\\prime}(x)  tells us how much  f(x)  changes when  x  changes slightly.\n",
    "\n",
    "\tExample:\n",
    "\tSuppose  f(x) = x^2 :\n",
    "\t•\tWhen  x = 2 ,  f'(x) = 2x = 4 \n",
    "\t•\tThis means that at  x = 2 , if  x  increases by a tiny amount (Delta x),  f(x)  will increase approximately by  4 * Delta(x)\n",
    "\n",
    "\tThe derivative gives us a local understanding of how changes in  x  affect  f(x) .\n",
    "\n",
    "\n",
    "\n",
    "The goal of training a neural network is to minimize the cost function (e.g., mean squared error or cross-entropy), which measures how far the model’s predictions are from the true labels.\n",
    "\n",
    "\t•\tTo find the optimal weights (W) that minimize the cost, we need to know:\n",
    "\t•\tIn which direction to adjust W.\n",
    "\t•\tHow much to adjust W.\n",
    "\t•\tThe derivative of the cost w.r.t. the weights provides this information:\n",
    "\n",
    "\\frac{\\partial \\text{Cost}}{\\partial W}\n",
    "\n",
    "\t•\tSign of the derivative: Determines the direction of adjustment.\n",
    "\t•\tIf positive, reduce the weight.\n",
    "\t•\tIf negative, increase the weight.\n",
    "\t•\tMagnitude of the derivative: Determines the size of the adjustment.\n",
    "\t•\tLarge derivative → larger change.\n",
    "\t•\tSmall derivative → smaller change.\n",
    "\n",
    "In a neural network, the cost function  C  depends on the weights  W  (and biases). Each weight contributes to the model’s output, which in turn affects the cost.\n",
    "\n",
    "The derivative  \\frac{\\partial C}{\\partial W}  tells us:\n",
    "\t•\tHow sensitive the cost is to a small change in a particular weight.\n",
    "\t•\tSpecifically, it answers: “If I slightly increase or decrease this weight, how will the cost change?”\n",
    "\n",
    "Why is this helpful?\n",
    "\n",
    "\t1.\tIf the cost decreases significantly when a weight increases, we want to increase that weight.\n",
    "\t2.\tIf the cost increases when a weight increases, we want to decrease that weight.\n",
    "\n",
    "\n",
    "\n",
    "Gradient descent also uses derivatives to iteratively adjust the weights:\n",
    "\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Cost}}{\\partial W}\n",
    "\n",
    "\t•\tBy repeatedly moving in the direction of the negative gradient, the algorithm converges toward the minimum of the cost function.\n",
    "\n",
    "\n",
    "Measure Sensitivity\n",
    "\n",
    "\t•\tThe derivative tells us how sensitive the cost is to changes in a specific weight or input. This helps identify which parameters have the most significant impact on the network’s performance.\n",
    "\n",
    "\n",
    "\n",
    "Chain Rule in Backpropagation :\n",
    "\n",
    "\t•\tIn a neural network,  C  depends on  W  indirectly through intermediate layers:\n",
    "\n",
    "\tw -> z -> a -> c \n",
    "\n",
    "\t•\tC = f(g(h(W)))\n",
    "\n",
    "\t•\tTo compute  \\frac{\\partial C}{\\partial W} , we use the chain rule:\n",
    "\n",
    "\t•\t\\frac{\\partial C}{\\partial W} = \\frac{\\partial C}{\\partial h} \\cdot \\frac{\\partial h}{\\partial g} \\cdot \\frac{\\partial g}{\\partial W}\n",
    "\n",
    "\t•\tThis ensures the gradient accounts for all layers and connections between  W  and  C .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. compile\n",
    "\n",
    "The optimizer is responsible for updating the weights during training to minimize the loss function.\n",
    "\n",
    "Adam (used in this code):\n",
    "\n",
    "•\tAdam (Adaptive Moment Estimation) combines:\n",
    "\t•\tMomentum: Accelerates convergence by considering past gradients.\n",
    "\t•\tAdaptive Learning Rates: Scales learning rates for each weight individually based on past gradients.\n",
    "\n",
    "    •\tWhy it matters: Adam automatically adjusts the learning rate for each weight based on the gradient history. This is particularly useful in CNNs, where:\n",
    "\t\n",
    "    •\tGradients might vary significantly across layers (e.g., early layers vs. deeper layers).\n",
    "\t\n",
    "    •\tFeatures extracted in convolutional layers may lead to diverse scales of gradient values.\n",
    "\t\n",
    "    •\tBenefit for your project: Since your CNN involves multiple blocks (block1 to block4), adaptive learning rates ensure stable and efficient updates across all layers without requiring manual tuning.\n",
    "\n",
    "    •\tEach weight in the network (whether in block1, block2, or block4) gets its own learning rate that adjusts dynamically during training.\n",
    "\t\n",
    "    •\tAdam essentially scales the influence of the gradient based on how it behaved in previous iterations, ensuring smoother and more stable updates.\n",
    "\t\n",
    "\n",
    "•\tParameters:\n",
    "\t•\tlearning_rate = 1e-3: Initial step size for weight updates.\n",
    "\t•\tCommon values: 10^{-2}, 10^{-3}, 10^{-4}.\n",
    "    \n",
    "    •\tHigher values (e.g., 1e-2) can lead to faster initial learning but risk overshooting the optimal weights.\n",
    "\t\n",
    "    •\tLower values (e.g., 1e-4) result in slower training, which might be unnecessary given Adam’s built-in safeguards against large gradient steps.\n",
    "\n",
    "•\tOther Adam-specific parameters:\n",
    "\t•\tbeta_1=0.9: Exponential decay rate for the first moment estimate.\n",
    "\t•\tbeta_2=0.999: Exponential decay rate for the second moment estimate.\n",
    "\t•\tepsilon=1e-7: Small value to prevent division by zero.\n",
    "\n",
    "Other Common Optimizers:\n",
    "\n",
    "•\tSGD (Stochastic Gradient Descent):\n",
    "\t•\tUpdates weights using a fixed learning rate.\n",
    "\t•\tParameters:\n",
    "\t•\tlearning_rate: Fixed step size.\n",
    "\t•\tmomentum: Adds inertia to updates.\n",
    "\t•\tExample: optimizer=SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    •\tSimple Approach: Updates weights based on the gradient of the loss function with respect to weights.\n",
    "\t\n",
    "    •\tLearning Rate Scheduling: Requires careful tuning of the learning rate, often needing decay strategies for optimal results.\n",
    "\t\n",
    "    •\tMomentum (Optional): Can include momentum to accelerate convergence.\n",
    "\n",
    "When to Choose What?\n",
    "\n",
    "Adam:\n",
    "\n",
    "\t•\tWhen you’re working with:\n",
    "\t•\tA deep architecture (e.g., Transformers, CNNs, RNNs).\n",
    "\t•\tSparse data or embeddings (e.g., language models, recommendation systems).\n",
    "\t•\tA new problem where hyperparameter tuning is challenging.\n",
    "\n",
    "SGD:\n",
    "\n",
    "\t•\tWhen:\n",
    "\t•\tYou can invest time in tuning learning rates and momentum.\n",
    "\t•\tYou prioritize generalization over faster convergence.\n",
    "\t•\tWorking on image tasks like ResNet or models with batch normalization.\n",
    "\n",
    "\n",
    "\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "\n",
    "What Does It Mean?\n",
    "\n",
    "\t•\tThe loss function is the measure of how well the model’s predictions match the true labels. During training, the optimizer minimizes this loss.\n",
    "\t•\tsparse_categorical_crossentropy is a specific type of loss function used for classification problems where:\n",
    "\t•\tThe target labels are integers (e.g., 0, 1, 2, …).\n",
    "\t•\tThe model outputs probabilities for each class via a softmax activation.\n",
    "\n",
    "Why Sparse?\n",
    "\n",
    "\t•\tSparse means the target labels are provided as integers instead of one-hot encoded vectors.\n",
    "\t•\tExample: If you have 3 classes, a target label of 1 would represent the class [0, 1, 0] in one-hot encoding.\n",
    "\t•\tsparse_categorical_crossentropy saves computation and memory by working directly with integer labels.\n",
    "\n",
    "\n",
    "\n",
    "metrics = ['accuracy']\n",
    "\n",
    "What Does It Mean?\n",
    "\n",
    "\t•\tMetrics are used to evaluate the performance of your model during training and validation. Unlike the loss function, metrics don’t influence training directly but provide useful feedback.\n",
    "\n",
    "Why Accuracy?\n",
    "\n",
    "\t•\tAccuracy is the percentage of correctly classified samples:\n",
    "\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "\n",
    "\t•\tFor classification tasks, it’s a straightforward and interpretable metric.\n",
    "\n",
    "When to Use:\n",
    "\n",
    "\t•\tUse accuracy for balanced datasets where each class has roughly the same number of examples.\n",
    "\t•\tIf your dataset is imbalanced, consider alternative metrics like precision, recall, or F1-score.\n",
    "\n",
    "\n",
    "\n",
    "2. EarlyStopping\n",
    "\n",
    "•\tWhat It Does:\n",
    "\t•\tMonitors the model’s performance on the validation dataset (val_loss in this case).\n",
    "\t•\tStops training early if the monitored metric (val_loss) does not improve for a specified number of epochs (patience).\n",
    "\t•\tPrevents overfitting and saves computation time by avoiding unnecessary training epochs.\n",
    "\t\n",
    "•\tKey Parameters:\n",
    "\t•\tmonitor: The metric to track. Common choices are:\n",
    "\t•\t'val_loss': Validation loss (used here).\n",
    "\t•\t'val_accuracy': Validation accuracy.\n",
    "\n",
    "•\tpatience: Number of epochs to wait for improvement before stopping training.\n",
    "\t•\tIn this case, training will stop if val_loss does not improve for 25 consecutive epochs.\n",
    "\n",
    "•\trestore_best_weights: Ensures the weights from the best epoch are used at the end of training.\n",
    "\t•\tPrevents the model from ending with weights from a less optimal epoch.\n",
    "\n",
    "3. ReduceLROnPlateau\n",
    "\n",
    "•\tWhat It Does:\n",
    "\t•\tDynamically adjusts the learning rate during training if the model’s performance plateaus (stops improving).\n",
    "\t•\tReducing the learning rate allows the model to make smaller, more precise adjustments to the weights, which can help it converge better.\n",
    "\t\n",
    "•\tKey Parameters:\n",
    "\t•\tmonitor: The metric to track. Like EarlyStopping, this is set to 'val_loss' here.\n",
    "\t\n",
    "\t•\tfactor: The factor by which the learning rate is reduced.\n",
    "\t\t•\tIn this case, the learning rate is halved (factor=0.5) when the validation loss stops improving.\n",
    "\t\n",
    "\t•\tpatience: Number of epochs to wait before reducing the learning rate.\n",
    "\t\t•\tIf val_loss does not improve for 10 epochs, the learning rate is reduced.\n",
    "\t\n",
    "\t•\tmin_lr: The minimum learning rate allowed.\n",
    "\t\t•\tPrevents the learning rate from becoming too small and stopping useful updates.\n",
    "\t\t•\tHere, it’s set to 1e-6.\n",
    "\t\n",
    "\t•\tverbose: Controls the verbosity.\n",
    "\t\t•\tverbose=1 prints a message when the learning rate is reduced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters for DeepConvNet\n",
    "# Binary classification\n",
    "nb_classes = 2  \n",
    "# Chans: Number of channels.\n",
    "Chans = X.shape[1]\n",
    "# Samples: Number of time samples.\n",
    "Samples = X.shape[2]\n",
    "\n",
    "# Compile the model\n",
    "model = DeepConvNet(nb_classes = nb_classes, \n",
    "                    Chans = Chans, \n",
    "                    Samples = Samples)\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate = 1e-3),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Adjust EarlyStopping and ReduceLROnPlateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=25, \n",
    "                               restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.5, \n",
    "                              patience=10,\n",
    "                              min_lr=1e-6, \n",
    "                              verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. epochs\n",
    "\n",
    "•\tDefinition:\n",
    "\t•\tThe number of full passes over the training dataset during training.\n",
    "\t•\tEach epoch involves training the model on the entire dataset once.\n",
    "\t\n",
    "•\tValue Set:\n",
    "\t•\tepochs = 500 means the model will go through the dataset up to 500 times unless interrupted by EarlyStopping.\n",
    "\t\n",
    "•\tWhy 500?:\n",
    "\t•\tIt’s a high enough value to ensure the model has ample opportunity to learn.\n",
    "\t•\tIn practice, EarlyStopping will likely terminate the training before reaching 500 epochs if the validation loss stops improving.\n",
    "\n",
    "The epochs in the context of training a neural network refer to how many times the entire training dataset is passed through the model, not the number of individual data points in the dataset.\n",
    "\n",
    "Backpropagation will occur once per epoch, so if you have 500 epochs, backpropagation will be applied 500 times.\n",
    "\n",
    "•\tEpochs: One epoch means that the model has seen every data point in the training set once.\n",
    "\t\n",
    "    •\tFor example, if you have a dataset with 13,000 samples, and you set epochs = 500, this means the model will go through the 13,000 samples 500 times during training.\n",
    "\n",
    "    •\tWhen you train the model, it will process all 13,000 samples in one epoch.\n",
    "\n",
    "\t•\tAfter completing the first pass (epoch) over the data, the model will adjust the weights using backpropagation and then start the next epoch, repeating this 500 times.\n",
    "\n",
    "In one epoch, you:\n",
    "\t1.\tDo the forward pass to get predictions.\n",
    "\t2.\tCompute the loss based on the predictions.\n",
    "\t3.\tPerform backpropagation to compute the gradients of the loss with respect to the model’s weights.\n",
    "\t4.\tUse gradient descent (or Adam, or other optimizers) to update the weights.\n",
    "\n",
    "2. batch_size\n",
    "\n",
    "•\tBatch size is the number of samples from your training dataset that the model will process simultaneously in one forward and backward pass before the weights are updated.\n",
    "\n",
    "In your case, with batch_size = 16, the model will:\n",
    "\t1.\tTake 16 samples from the training dataset (e.g., 16 images or 16 data points).\n",
    "\t2.\tPerform a forward pass through the network with these 16 samples.\n",
    "\t3.\tCompute the loss (error) for all 16 samples.\n",
    "\t4.\tPerform backpropagation to calculate the gradients of the loss for each of the weights, considering all 16 samples at once.\n",
    "\t5.\tUse the optimizer (e.g., Adam) to update the weights based on the calculated gradients.\n",
    "\n",
    "After these steps, the model will have completed one batch of training, and then it will move on to the next batch of 16 samples, continuing the training process for the entire dataset.\n",
    "\n",
    "Why Batch Size Matters:\n",
    "\n",
    "•\tSmaller Batch Sizes (like 16):\n",
    "\t•\tPros:\n",
    "\t•\tFaster computation per batch since fewer samples are processed at a time.\n",
    "\t•\tCan provide a more accurate estimate of the gradients for each batch.\n",
    "\t•\tHelps in better generalization by adding more noise in the gradient estimates (this can help avoid overfitting).\n",
    "\t\n",
    "    •\tCons:\n",
    "\t•\tThe updates to the weights can be noisier and less stable, as the gradient estimate might be less accurate for smaller batches.\n",
    "\n",
    "•\tLarger Batch Sizes:\n",
    "\t•\tPros:\n",
    "\t•\tMore stable gradient updates as the weight updates are computed over a larger number of samples.\n",
    "\t•\tCan potentially speed up training since fewer updates are needed for each epoch.\n",
    "\t\n",
    "    •\tCons:\n",
    "\t•\tCan be more computationally expensive and require more memory.\n",
    "\t•\tMight overfit more since the gradients are less noisy.\n",
    "\n",
    "Given 13,000 samples and a batch size of 16, the dataset will be divided into batches, each containing 16 samples. For each epoch, the model will process all of these batches sequentially. \n",
    "\n",
    "When batch normalization is used in the model, it is applied independently to each batch rather than to the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "epochs = 500\n",
    "batch_size = 16  # Keep batch size small for better gradient estimation\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# split the dataset into training and test sets, ensuring that the model can be trained on one portion of the data and evaluated on another to check its generalization performance.\n",
    "# test_size=0.2:\n",
    "    # This parameter specifies the proportion of the dataset to be used for the test set.\n",
    "    # 0.2 means 20% of the data will be used for testing, and the remaining 80% will be used for training (the model will be trained on 80% of the data and tested on 20%).\n",
    "# random_state=42:\n",
    "    # This is a seed for the random number generator that ensures the split is reproducible. If you run the code multiple times with the same random_state, you will get the same split of data each time.\n",
    "    # It’s a good practice to set a random_state when you want to ensure that your results can be replicated exactly.\n",
    "# stratify=y_aug:\n",
    "    # This ensures that the distribution of classes in the training and test sets is similar to the distribution in the original dataset (y_aug).\n",
    "    # For example, if your data has an equal number of classes, stratification ensures that both the training and test sets will have roughly the same proportions of each class.\n",
    "    # This is especially useful in imbalanced datasets, where one class might be underrepresented. Without stratification, the test set might end up with very few or no samples of a certain class, which would hurt the model evaluation.\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_aug, \n",
    "                                                            y_aug, \n",
    "                                                            test_size = 0.2, \n",
    "                                                            random_state = 42, \n",
    "                                                            stratify = y_aug)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# validation_data=(X_test, y_test):\n",
    "\t# During training, the model will also evaluate its performance on the validation data (X_test, y_test) at the end of each epoch (i.e., after each pass through the training data).\n",
    "\t# The validation data is used to track how well the model is generalizing to unseen data, and it’s crucial for monitoring overfitting.\n",
    "\t# The loss and accuracy on the validation data will be calculated and printed alongside the training loss and accuracy.\n",
    "history = model.fit(X_train_val, \n",
    "                    y_train_val,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data = (X_test, y_test),\n",
    "                    callbacks = [early_stopping, reduce_lr],\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and Visualization\n",
    "# Evaluates the trained model on the test data (X_test) and the corresponding true labels (y_test).\n",
    "# This function calculates two things:\n",
    "\t# Test Loss: The loss value on the test data (how well the model’s predictions match the true labels).\n",
    "\t# Test Accuracy: The accuracy of the model on the test data (how many predictions were correct).\n",
    "# verbose=0: This suppresses output during evaluation. If set to 1, it will display the progress bar.\n",
    "# print(f'Test Accuracy: {test_accuracy * 100:.2f}%'): This prints the test accuracy, multiplied by 100 to express it as a percentage.\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# Visualize training history\n",
    "# Plot accuracy\n",
    "plt.figure(figsize = (12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label = 'Train Accuracy', color = 'blue')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy', color = 'orange')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label = 'Train Loss', color = 'blue')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss', color = 'orange')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create the 'figures' directory if it does not exist\n",
    "if not os.path.exists('figures'):\n",
    "    os.makedirs('figures')\n",
    "    \n",
    "# Save the figure to the 'figures' directory in the current working directory\n",
    "plt.savefig('figures/model_accuracy_loss.png')\n",
    "# plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "# This generates the predicted probabilities for each class for the test samples. \n",
    "# Since the model output is a probability distribution over the classes (using the softmax activation function in the final layer), the output y_pred_prob is a 2D array with the shape (num_samples, num_classes).\n",
    "y_pred_prob = model.predict(X_test)\n",
    "# This finds the class with the highest predicted probability for each sample. \n",
    "# The axis=1 means it looks across the columns (the class probabilities for each sample), and np.argmax returns the index of the highest value (which corresponds to the predicted class). \n",
    "# The result is a 1D array of predicted class labels (y_pred).\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "# This calculates the confusion matrix, which compares the true labels (y_test) to the predicted labels (y_pred). \n",
    "# The confusion matrix is a table used to describe the performance of a classification model. \n",
    "# It shows the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, \n",
    "            annot = True, \n",
    "            fmt = 'd', \n",
    "            cmap = 'Blues',\n",
    "            xticklabels = ['Face', 'Car'],\n",
    "            yticklabels = ['Face', 'Car'])\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save the figure to the 'figures' directory in the current working directory\n",
    "plt.savefig('figures/confusion_matrix.png')\n",
    "\n",
    "# plt.close()\n",
    "\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['Face', 'Car'])\n",
    "\n",
    "print(report)\n",
    "\n",
    "\n",
    "# Save the classification report to a text file\n",
    "with open('classification_report.txt', 'w') as f:\n",
    "    f.write(report)  # Write the report to the file\n",
    "\n",
    "\n",
    "model.save('figures/V2.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n170",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
